<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Minería de Datos - 7&nbsp; Aprendizaje supervisado</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./reglas.html" rel="next">
<link href="./eval.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "Sín resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la busqueda",
    "search-hide-matches-text": "Esconder resultados adicionales",
    "search-more-match-text": "hay más resultados en este documento",
    "search-more-matches-text": "más resultados en este documento",
    "search-clear-button-title": "Borrar",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Eviar"
  }
}</script>
<script src="site_libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="site_libs/viz-1.8.2/viz.js"></script>
<link href="site_libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet">
<script src="site_libs/grViz-binding-1.0.10/grViz.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Aprendizaje supervisado</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Minería de Datos</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Prefacio</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducción</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Datos</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eda.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Análisis Exploratorio de Datos</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reddim.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Técnicas de reducción de la dimensionalidad</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./aprnosup.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aprendizaje no supervisado</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eval.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Medidas de rendimiento</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./aprsup.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Aprendizaje supervisado</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reglas.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Reglas de asociación</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nuevas.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nuevas tendencias</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusiones.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Conclusiones</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">Bibliografía</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Indice de contenidos</h2>
   
  <ul>
  <li><a href="#modelos-lineales" id="toc-modelos-lineales" class="nav-link active" data-scroll-target="#modelos-lineales"> <span class="header-section-number">7.1</span> Modelos Lineales</a>
  <ul class="collapse">
  <li><a href="#modelos-lineales-generalizados" id="toc-modelos-lineales-generalizados" class="nav-link" data-scroll-target="#modelos-lineales-generalizados"> <span class="header-section-number">7.1.1</span> Modelos lineales generalizados</a></li>
  <li><a href="#regresión-logística" id="toc-regresión-logística" class="nav-link" data-scroll-target="#regresión-logística"> <span class="header-section-number">7.1.2</span> Regresión Logística</a></li>
  <li><a href="#probabilidad-de-clase" id="toc-probabilidad-de-clase" class="nav-link" data-scroll-target="#probabilidad-de-clase"> <span class="header-section-number">7.1.3</span> Probabilidad de clase</a></li>
  <li><a href="#análisis-discriminante-lineal" id="toc-análisis-discriminante-lineal" class="nav-link" data-scroll-target="#análisis-discriminante-lineal"> <span class="header-section-number">7.1.4</span> Análisis Discriminante Lineal</a></li>
  </ul></li>
  <li><a href="#k-vecinos" id="toc-k-vecinos" class="nav-link" data-scroll-target="#k-vecinos"> <span class="header-section-number">7.2</span> k-Vecinos</a></li>
  <li><a href="#grid-search" id="toc-grid-search" class="nav-link" data-scroll-target="#grid-search"> <span class="header-section-number">7.3</span> Grid search</a></li>
  <li><a href="#árboles-de-decisión" id="toc-árboles-de-decisión" class="nav-link" data-scroll-target="#árboles-de-decisión"> <span class="header-section-number">7.4</span> Árboles de Decisión</a></li>
  <li><a href="#métodos-de-ensamblado" id="toc-métodos-de-ensamblado" class="nav-link" data-scroll-target="#métodos-de-ensamblado"> <span class="header-section-number">7.5</span> Métodos de ensamblado</a>
  <ul class="collapse">
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging"> <span class="header-section-number">7.5.1</span> Bagging</a></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting"> <span class="header-section-number">7.5.2</span> Boosting</a></li>
  </ul></li>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes"> <span class="header-section-number">7.6</span> Naive Bayes</a></li>
  <li><a href="#modelos-de-mezcla-de-gaussianas" id="toc-modelos-de-mezcla-de-gaussianas" class="nav-link" data-scroll-target="#modelos-de-mezcla-de-gaussianas"> <span class="header-section-number">7.7</span> Modelos de mezcla de Gaussianas</a></li>
  <li><a href="#comparación-de-modelos" id="toc-comparación-de-modelos" class="nav-link" data-scroll-target="#comparación-de-modelos"> <span class="header-section-number">8</span> Comparación de modelos</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-aprsup" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Aprendizaje supervisado</span></span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>El aprendizaje automático supervisado es una disciplina fundamental en la ciencia de datos y la IA. En este enfoque, trabajamos con un conjunto de datos en el que conocemos la variable objetivo que deseamos predecir o clasificar. Esta variable objetivo contiene la información que queremos explicar o entender, y su comportamiento se basa en el resto de las variables o características del conjunto de datos. El objetivo principal del ML supervisado es desarrollar modelos que puedan capturar patrones y relaciones entre las variables con el fin de realizar predicciones precisas y clasificar observaciones.</p>
<p>Uno de los aspectos más destacados del ML supervisado es su capacidad para resolver problemas de clasificación, donde se asignan observaciones a diferentes categorías o clases. Por ejemplo, podemos aplicar modelos de clasificación para predecir si un correo electrónico es spam o legítimo, si un paciente tiene una enfermedad específica o no, o si una transacción bancaria es fraudulenta o no.</p>
<p>Además de la clasificación, el ML supervisado también se utiliza para abordar problemas de regresión, donde la variable objetivo es una cantidad continua. Esto nos permite realizar predicciones numéricas, como predecir el precio de una casa en función de sus características, pronosticar la demanda de productos en función de variables de mercado, o estimar la duración de un proyecto en función de factores diversos.</p>
<p>En este tema, exploraremos diversos algoritmos de clasificación supervisada que permiten extraer patrones a partir de los datos y realizar predicciones precisas en una variedad de aplicaciones. Aprenderemos cómo entrenar modelos, evaluar su rendimiento y aplicarlos a situaciones del mundo real. El ML supervisado es una herramienta poderosa que puede proporcionar conocimientos valiosos y respuestas a preguntas importantes en una amplia gama de dominios.</p>
<section id="modelos-lineales" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="modelos-lineales"><span class="header-section-number">7.1</span> Modelos Lineales</h2>
<p>Los modelos lineales representan uno de los pilares más robustos y extensamente empleados en el ámbito del aprendizaje supervisado en ML. Estos modelos constituyen la base de numerosas aplicaciones de predicción y clasificación, desempeñando un papel fundamental en la comprensión de los principios subyacentes de los algoritmos de ML.</p>
<p>La premisa fundamental de un modelo lineal es la suposición de que existe una relación lineal entre las variables de entrada (características) y la variable objetivo que se desea predecir. En otras palabras, se busca identificar una combinación lineal de las características ponderadas por coeficientes que se ajuste óptimamente a los datos observados. Los modelos lineales destacan por su alta interpretabilidad, lo que facilita un análisis claro sobre cómo cada característica influye en la variable objetivo.</p>
<p>En este capítulo exploraremos los modelos lineales, iniciando con los modelos lineales generalizados, entre los cuales el modelo de regresión lineal representa un caso específico. Además, examinaremos modelos lineales de clasificación, como la regresión logística, que se emplean para abordar problemas de clasificación.</p>
<section id="modelos-lineales-generalizados" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="modelos-lineales-generalizados"><span class="header-section-number">7.1.1</span> Modelos lineales generalizados</h3>
<p>Los Modelos Lineales Generalizados (<strong>GLM</strong>, de “<em>Generalized Linear Models</em>”) son una extensión poderosa de los modelos lineales tradicionales que amplían su aplicabilidad a una variedad de problemas más complejos en el campo del ML. Si necesitáis una referencia completa a este tipo de modelos: <span class="citation" data-cites="mccullagh2019generalized">(<a href="references.html#ref-mccullagh2019generalized" role="doc-biblioref">McCullagh 2019</a>)</span>.</p>
<p>Estos modelos son aplicables tanto a variables respuesta cuantitativas como cualitativas. De hecho, a diferencia de los modelos lineales simples, los GLM permiten manejar una amplia gama de situaciones, incluyendo variables de respuesta que no siguen una distribución normal, relaciones no lineales entre las características y la variable objetivo, y variables categóricas. Los GLM se basan en tres componentes clave:</p>
<ul>
<li>la función de enlace,</li>
<li>la distribución de probabilidad,</li>
<li>la estructura lineal.</li>
</ul>
<p>Llamamos <span class="math inline">\(y\)</span> a la variable respuesta. Además, se define un predictor lineal con la combinación lineal ponderada de las <span class="math inline">\(p\)</span> variables explicativas <span class="math inline">\(x_1, x_2, ..., x_p\)</span>:</p>
<p><span class="math display">\[\eta (x)=\beta_0+\beta_1x_1+\ldots+\beta_px_p. \]</span></p>
<p>La <strong>función de enlace</strong> conecta la media de la variable respuesta con la combinación lineal de las características (el predictor lineal), lo que permite modelar relaciones no lineales y capturar la variabilidad de los datos. La elección de la función de enlace depende del tipo de problema que se esté abordando y puede incluir funciones como la logística para problemas de clasificación o la identidad para problemas de regresión.</p>
<p>La <strong>estructura lineal</strong> implica que las características se ponderan por coeficientes, similar a los modelos lineales tradicionales, pero con la capacidad de ajustar relaciones no lineales y manejar múltiples predictores, incluyendo variables categóricas.</p>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Repaso
</div>
</div>
<div class="callout-body-container callout-body">
<p>Quizás es un buen momento para repasar los modelos lineales (regresión) que hayas visto en otras asignaturas del grado.</p>
</div>
</div>
<p>Definimos una función de enlace que relaciona el valor esperado de la variable respuesta dadas las variables explicativas <span class="math inline">\(E(y|x)=\mu(x)\)</span>, con el predictor lineal:</p>
<p><span class="math display">\[g(E(y|x))=\eta(x)\]</span></p>
<p>La <strong>distribución de probabilidad</strong> (componente aleatoria) describe cómo se distribuyen los datos alrededor de la media y puede adaptarse a diferentes tipos de datos, como datos binarios, Poisson, binomiales o exponenciales, entre otros. Esto brinda flexibilidad para modelar una variedad de escenarios de datos. Por ejemplo, en el caso de la conocida <em>regresión lineal</em> se asume que la variable respuesta sigue una distribución <em>Normal</em>. Mediante la componente aleatoria determinamos cómo añadir el ruido o error aleatorio a la predicción que se obtiene de la función de enlace. La tabla siguiente, adaptada de <span class="citation" data-cites="agresti2015foundations">(<a href="references.html#ref-agresti2015foundations" role="doc-biblioref">Agresti 2015</a>)</span>, resume los GLM:</p>
<table class="table">
<colgroup>
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 20%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Modelo</th>
<th>Componente aleatoria</th>
<th>Función de enlace</th>
<th>Tipo de variables explicativas</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regresión Lineal</td>
<td>Normal</td>
<td>Identidad</td>
<td>Cuantitativas</td>
</tr>
<tr class="even">
<td>ANOVA</td>
<td>Normal</td>
<td>Identidad</td>
<td>Cuantitativas</td>
</tr>
<tr class="odd">
<td>ANCOVA</td>
<td>Normal</td>
<td>Identidad</td>
<td>Cuantitativas y Cualitativas</td>
</tr>
<tr class="even">
<td>Regresión Logística</td>
<td>Binomial</td>
<td>Logit</td>
<td>Cuantitativas y Cualitativas</td>
</tr>
<tr class="odd">
<td>LogLinear</td>
<td>Poisson</td>
<td>Log</td>
<td>Cualitativas</td>
</tr>
<tr class="even">
<td>Regresión de Poisson</td>
<td>Poisson</td>
<td>Log</td>
<td>Cuantitativas y Cualitativas</td>
</tr>
</tbody>
</table>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Para recordar
</div>
</div>
<div class="callout-body-container callout-body">
<p>Los GLM no asumen una relación lineal entre la variable respuesta y las variables explicativas (o independientes), pero sí suponen una relación lineal entre la variable respuesta transformada en términos de la función de enlace y las variables explicativas.</p>
</div>
</div>
</section>
<section id="regresión-logística" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="regresión-logística"><span class="header-section-number">7.1.2</span> Regresión Logística</h3>
<p>El modelo de Regresión Logística es uno de los modelos lineales generalizados más empleado en la práctica por la facilidad en la interpretación de sus resultados. Es un modelo adecuado para modelar la relación entre una variable respuesta binaria (<span class="math inline">\(0\)</span> o <span class="math inline">\(1\)</span>) y un conjunto de variables explicativas cuantitativas y/o cualitativas.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Multiclase
</div>
</div>
<div class="callout-body-container callout-body">
<p>Existe una versión de la Regresión Logística adaptada al caso de una variable respuesta con más de dos categorías, que recibe el nombre de Regresión Logística Politómica o Multinomial.</p>
</div>
</div>
<p>Asumiremos que la variables explicativas son independientes entre sí y que la probabilidad de <span class="math inline">\(1\)</span> (a veces llamada probabilidad de éxito) solo depende de los valores de dichas variables. Si las variables explicativas no fueran independientes, podríamos añadir interacciones al modelo.</p>
<p>Tratamos de modelar la probabilidad condicionada, <span class="math inline">\(\mu(x)\)</span>, de que la variable respuesta sea un <span class="math inline">\(1\)</span> dados los valores de las variables explicativas. En este caso, la componente de la variables respuesta se corresponde con una distribución binomial, <span class="math inline">\(Binomial(n,\pi)\)</span>, donde <span class="math inline">\(\pi=\mu(x)\)</span> es la probabilidad de éxito. Así,</p>
<p><span class="math display">\[  g(E(y|x)=g(μ(x))=\eta(x) =\]</span> <span class="math display">\[=logit(\mu(x))=log\left(\frac{\mu(x)}{(1-\mu(x))}\right)=log\left(\frac{E(y|x)}{1-E(y|x)}\right)\]</span> <span class="math display">\[=\beta_0+\beta_1x_1+\ldots+\beta_px_p\]</span></p>
<p>que modela la log odds de probabilidad de <span class="math inline">\(1\)</span> como función de las variables explicativas. Esto es, se modela el logaritmo de la probabilidad de <span class="math inline">\(1\)</span> dadas las variables explicativas, frente a la probabilidad de <span class="math inline">\(0\)</span> dadas las variables explicativas.</p>
<p>De este modo:</p>
<p><span class="math display">\[\mu(x)=\frac{1}{1+exp(-\eta(x))}=\frac{1}{1+exp(-(\beta_0+\beta_1x_1+...+\beta_px_p))}\]</span></p>
<p>Se emplean métodos de Máxima Verosimilitud para la estimación de los parámetros <span class="math inline">\(\beta_i, i=0,...,p\)</span>. Es posible realizar contrastes de hipótesis sobre dichos parámetros, tratando de eliminar del modelo las variables no significativas. Es decir, se contrasta si en los datos hay suficiente información contraria a la siguiente hipótesis nula, como para rechazarla: <span class="math display">\[H_o:\beta_i=0\]</span> La interpretación de los coeficientes se realiza mediante una razón de ventajas (“<em>odds ratio</em>” en inglés), tomando la exponencial de los estimadores de los parámetros del modelo.</p>
<p>Con los parámetros estimados, y sus desviaciones típicas, podremos hacer selección de variables, quedándonos con las más significativas (aquellas con un <span class="math inline">\(p-valor\)</span> por debajo de un umbral).</p>
<p>Vamos a trabajar con el ejemplo de los bancos que se estudió en el <a href="eda.html"><span>Capítulo&nbsp;3</span></a>. Como primera aproximación a la regresión logística, aplicamos un modelo con una única variable explicativa. En este caso elegimos la variable <code>housing</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Lectura de datos</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>bank <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">'https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(bank)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 11162    17</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>bank<span class="ot">=</span><span class="fu">as.tibble</span>(bank)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Parciticionamos los datos</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2138</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>n<span class="ot">=</span><span class="fu">dim</span>(bank)[<span class="dv">1</span>]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>indices<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1</span><span class="sc">:</span>n)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>indices.train<span class="ot">=</span><span class="fu">sample</span>(indices,<span class="at">size=</span>n<span class="sc">*</span>.<span class="dv">5</span>,<span class="at">replace=</span><span class="cn">FALSE</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>indices.test<span class="ot">=</span><span class="fu">sample</span>(indices[<span class="sc">-</span>indices.train],<span class="at">size=</span>n<span class="sc">*</span>.<span class="dv">25</span>,<span class="at">replace=</span><span class="cn">FALSE</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>indices.valid<span class="ot">=</span>indices[<span class="sc">-</span><span class="fu">c</span>(indices.train,indices.test)]</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>bank.train<span class="ot">=</span>bank[indices.train,]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>bank.test<span class="ot">=</span>bank[indices.test,]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>bank.valid<span class="ot">=</span>bank[indices.valid,]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Estudiamos la influencia de housing en deposit</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>tabla1<span class="ot">=</span><span class="fu">xtabs</span>(<span class="sc">~</span>housing<span class="sc">+</span>deposit,<span class="at">data=</span>bank.train)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>tabla1</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       deposit
housing   no  yes
    no  1246 1688
    yes 1662  985</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(tabla1)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Pearson's Chi-squared test with Yates' continuity correction

data:  tabla1
X-squared = 229.44, df = 1, p-value &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Podemos observar que existe una relación relevante entre ambas variables. El <span class="math inline">\(p-valor\)</span> es, claramente, inferior al valor de referencia <span class="math inline">\(0.05\)</span>. Empleamos la regresión logística para cuantificar la relación:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>factor.deposit <span class="ot">&lt;-</span> <span class="fu">factor</span>(bank.train<span class="sc">$</span>deposit)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>logit1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(factor.deposit <span class="sc">~</span> housing, <span class="at">data =</span> bank.train, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit1)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = factor.deposit ~ housing, family = "binomial", 
    data = bank.train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3088  -0.9648  -0.9648   1.0515   1.4061  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.30361    0.03735   8.129 4.34e-16 ***
housingyes  -0.82674    0.05488 -15.064  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7727  on 5580  degrees of freedom
Residual deviance: 7495  on 5579  degrees of freedom
AIC: 7499

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>¿Qué vemos en la salida? En primer lugar, los 3 asteriscos (” *** “) en la fila correspondiente a la variable <code>housing</code> indican su alta significatividad estadística. En otras palabras, es una variable muy relacionada con la respuesta (como ya habíamos averiguado). El valor del parámetro <span class="math inline">\(\beta_1\)</span> es de <span class="math inline">\(-0.82674\)</span> con una error estándar de <span class="math inline">\(0.05488\)</span>. ¿Qué significa? Es complicado interpretar valores negativos en los coeficientes. Para una mayor comprensión vamos a darle la vuelta. Es decir, el valor <span class="math inline">\(-0.82674\)</span> está asociado a”<em>housing=yes</em>”, de modo que el valor <span class="math inline">\(0.82674\)</span> está asociado a “<em>housing=no</em>”. La interpretación se realiza en base al exponente de dicho valor tal y como sigue: <span class="math display">\[exp(.82674)=2.29\]</span> significa que el “riesgo” de contratar un depósito es <span class="math inline">\(2.29\)</span> veces mayor en aquellos que no tienen casa, en relación con el mismo riesgo para aquellos que sí tienen casa en propiedad. Si hubíeramos mantenido la interpretación original (con el valor del coeficiente negativo), el resultado sería: “el riesgo de contratar un depósito es <span class="math inline">\(exp(-0.82674)=0.44\)</span> veces menor en aquellos propietarios de su propia casa, respecto a otros clientes que no tienen casa. Como puede verse es mucho más sencillo interpretar coeficientes positivos (odds ratio mayor que <span class="math inline">\(1\)</span>) que coeficientes negativos.</p>
<p>En base al valor del parámetro y a su error, podemos obtener intervalos de confianza:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR=</span><span class="fu">coef</span>(logit1),<span class="fu">confint.default</span>(logit1)))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                   OR     2.5 %   97.5 %
(Intercept) 1.3547352 1.2591064 1.457627
housingyes  0.4374726 0.3928586 0.487153</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint.default</span>(logit1)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 2.5 %     97.5 %
(Intercept)  0.2304023  0.3768097
housingyes  -0.9343056 -0.7191770</code></pre>
</div>
</div>
<p>Y por tanto, el riesgo de contratar un depósito es <span class="math inline">\(2.29 \in [2.05,2.54]\)</span> veces mayor en aquellos que no tienen casa, en relación con el mismo riesgo para aquellos que sí tienen casa en propiedad.</p>
<p>Vamos ahora a trabajar en el caso multivariante. Como primer paso estudiemos, una a una las variables que se van a incluir en el modelo. A modo de ejemplo, consideramos las siguientes variables:</p>
<ul>
<li><p><code>housing</code></p></li>
<li><p><code>marital</code></p></li>
<li><p><code>education</code></p></li>
<li><p><code>impago</code></p></li>
<li><p><code>saldo</code></p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># para que la categoría de referencia sea "married"</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>bank.train<span class="sc">$</span>marital<span class="ot">=</span><span class="fu">relevel</span>(<span class="fu">as.factor</span>(bank.train<span class="sc">$</span>marital),<span class="at">ref=</span><span class="dv">2</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>logit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(factor.deposit <span class="sc">~</span> marital, <span class="at">data =</span> bank.train, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = factor.deposit ~ marital, family = "binomial", 
    data = bank.train)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.260  -1.076  -1.076   1.200   1.283  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.24428    0.03575  -6.832 8.35e-12 ***
maritaldivorced  0.19139    0.08662   2.209   0.0271 *  
maritalsingle    0.43555    0.05974   7.290 3.09e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7727.0  on 5580  degrees of freedom
Residual deviance: 7673.4  on 5578  degrees of freedom
AIC: 7679.4

Number of Fisher Scoring iterations: 3</code></pre>
</div>
</div>
<p>La variable <code>marital</code> es estadísticamente significativa. Los clientes solteros tienen un riesgo del orden de <span class="math inline">\(1.5\)</span> veces mayor de <code>deposit=yes</code> respecto a los clientes casados. Los clientes divorciados tienen un riesgo del orden de <span class="math inline">\(1.2\)</span> veces mayor.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>logit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(factor.deposit <span class="sc">~</span> education, <span class="at">data =</span> bank.train, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = factor.deposit ~ education, family = "binomial", 
    data = bank.train)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.248  -1.106  -1.002   1.250   1.364  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.42876    0.07457  -5.750 8.92e-09 ***
educationsecondary  0.25901    0.08385   3.089 0.002008 ** 
educationtertiary   0.59332    0.08820   6.727 1.73e-11 ***
educationunknown    0.48005    0.14220   3.376 0.000736 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7727  on 5580  degrees of freedom
Residual deviance: 7671  on 5577  degrees of freedom
AIC: 7679

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>La variable <code>education</code> es estadísticamente significativa. A medida que la educación aumenta, los clientes tienen un riesgo mayor de <code>deposit=yes</code> (<span class="math inline">\(1\)</span> categoría de referencia, <span class="math inline">\(1.3\)</span> educación secundaria, <span class="math inline">\(1.8\)</span> educación terciaria). Los clientes con nivel de educación desconocido también tienen un riesgo positivo. Aquí la interpretación es más complicada al desconocer el nivel real de educación de estos clientes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>logit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(factor.deposit <span class="sc">~</span> default, <span class="at">data =</span> bank.train, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = factor.deposit ~ default, family = "binomial", 
    data = bank.train)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.144  -1.144  -1.144   1.211   1.382  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.07800    0.02701  -2.887  0.00388 **
defaultyes  -0.39200    0.21713  -1.805  0.07102 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7727.0  on 5580  degrees of freedom
Residual deviance: 7723.7  on 5579  degrees of freedom
AIC: 7727.7

Number of Fisher Scoring iterations: 3</code></pre>
</div>
</div>
<p>La variable <code>default</code> no es estadísticamente significativa. Su <span class="math inline">\(p-valor\)</span> es mayor que <span class="math inline">\(0.05\)</span>. Sin embargo, está cerca de dicho valor de referencia. Eliminar esta variable del modelo en una etapa tan temprana puede ser un error.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>logit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(factor.deposit <span class="sc">~</span> balance, <span class="at">data =</span> bank.train, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = factor.deposit ~ balance, family = "binomial", 
    data = bank.train)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-2.361  -1.118  -1.102   1.230   1.297  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.667e-01  3.036e-02  -5.489 4.04e-08 ***
balance      5.621e-05  1.004e-05   5.600 2.15e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7727.0  on 5580  degrees of freedom
Residual deviance: 7689.1  on 5579  degrees of freedom
AIC: 7693.1

Number of Fisher Scoring iterations: 3</code></pre>
</div>
</div>
<p>La variable <code>saldo</code> es claramente significativa. Un incremento de una unidad en el <code>saldo</code> está asociado a un incremento de <span class="math inline">\(1.00006\)</span> en el riesgo de depóstivo. En variables continuas es muy habitual que un incremento tan bajo (<span class="math inline">\(1\)</span> dolar esté asociado a incrementos tan pequeños en el riesgo). Para mejorar la interpretación se suele multiplicar esta cantidad como sigue:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(logit2<span class="sc">$</span>coefficients[<span class="dv">2</span>]<span class="sc">*</span><span class="dv">1000</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> balance 
1.057816 </code></pre>
</div>
</div>
<p>De este modo, la interpretación sería: los clientes con <span class="math inline">\(1000\)</span> dólares más en el salto incrementan su riesgo de depósito en <span class="math inline">\(1.06\)</span> respecto a los que tienen <span class="math inline">\(1000\)</span> dólares menos.</p>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Métodos Wrapper de selección de características
</div>
</div>
<div class="callout-body-container callout-body">
<p>En el <a href="reddim.html"><span>Capítulo&nbsp;4</span></a> vimos algunas técnicas de reducción de la dimensión basadas en la selección de características de acuerdo con la calidad de las mismas. Esta calidad se evalua empleando algoritmos de ML.</p>
</div>
</div>
<p>A continuación, ajustamos un modelo de regresión logística con todas las características significativas.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>logit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(factor.deposit <span class="sc">~</span> housing<span class="sc">+</span>marital<span class="sc">+</span>education<span class="sc">+</span>default<span class="sc">+</span> balance, <span class="at">data =</span> bank.train, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = factor.deposit ~ housing + marital + education + 
    default + balance, family = "binomial", data = bank.train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2899  -1.0945  -0.8051   1.1219   1.6448  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -1.982e-01  8.389e-02  -2.362  0.01815 *  
housingyes         -7.770e-01  5.585e-02 -13.912  &lt; 2e-16 ***
maritaldivorced     1.776e-01  8.901e-02   1.995  0.04604 *  
maritalsingle       3.728e-01  6.211e-02   6.002 1.95e-09 ***
educationsecondary  2.475e-01  8.666e-02   2.856  0.00429 ** 
educationtertiary   4.323e-01  9.166e-02   4.717 2.40e-06 ***
educationunknown    3.178e-01  1.464e-01   2.170  0.03000 *  
defaultyes         -2.920e-01  2.222e-01  -1.314  0.18872    
balance             4.495e-05  1.007e-05   4.462 8.12e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7727.0  on 5580  degrees of freedom
Residual deviance: 7397.1  on 5572  degrees of freedom
AIC: 7415.1

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>Podemos ver como la variable <code>default</code> pierde significatividad estadística. Esto es normal. Al tener en consideración el efecto de otras variables, aquellas que estaban al límite de la significatividad pueden ser eliminadas. Entrenamos el modelo sin esa variable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>logit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(factor.deposit <span class="sc">~</span> housing<span class="sc">+</span>marital<span class="sc">+</span>education<span class="sc">+</span> balance, <span class="at">data =</span> bank.train, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logit2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = factor.deposit ~ housing + marital + education + 
    balance, family = "binomial", data = bank.train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.3057  -1.0920  -0.8049   1.1217   1.6233  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -2.042e-01  8.377e-02  -2.438  0.01479 *  
housingyes         -7.772e-01  5.584e-02 -13.917  &lt; 2e-16 ***
maritaldivorced     1.751e-01  8.898e-02   1.968  0.04908 *  
maritalsingle       3.735e-01  6.209e-02   6.014 1.80e-09 ***
educationsecondary  2.479e-01  8.667e-02   2.861  0.00423 ** 
educationtertiary   4.328e-01  9.167e-02   4.721 2.35e-06 ***
educationunknown    3.151e-01  1.464e-01   2.152  0.03137 *  
balance             4.582e-05  1.009e-05   4.543 5.54e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7727.0  on 5580  degrees of freedom
Residual deviance: 7398.8  on 5573  degrees of freedom
AIC: 7414.8

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>Todas las variables del modelo son estadísticamente significativas. Si las variables fueran independientes, los coeficientes de la regresión múltiple coincidirían al 100% con los coeficientes de las regresiones simples. Por ejemplo, el coeficiente asociado a la variable <code>housing</code> era <span class="math inline">\(2.29\)</span> y ahora es <span class="math inline">\(2.18\)</span>, muy similar. Podemos intuir la existencia de una pequeña correlación entre las variables del modelo.</p>
</section>
<section id="probabilidad-de-clase" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="probabilidad-de-clase"><span class="header-section-number">7.1.3</span> Probabilidad de clase</h3>
<p>Podemos calcular la probabilidad que ofrece el modelo para cada una de las observaciones en la muestra de entrenamiento.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>predicciones <span class="ot">&lt;-</span> <span class="fu">predict</span>(logit2, <span class="at">type=</span><span class="st">"response"</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(predicciones)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Y podemos emplear esas predicciones, con un umbral de decisión ($0.5$ en el ejemplo), para clasificar.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>clase.pred<span class="ot">=</span><span class="fu">ifelse</span>(predicciones<span class="sc">&gt;</span><span class="fl">0.5</span>,<span class="st">"yes"</span>,<span class="st">"no"</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data=</span><span class="fu">as.factor</span>(clase.pred),<span class="at">reference=</span><span class="fu">as.factor</span>(bank.train<span class="sc">$</span>deposit),<span class="at">positive=</span><span class="st">"yes"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction   no  yes
       no  1790 1078
       yes 1118 1595
                                          
               Accuracy : 0.6065          
                 95% CI : (0.5936, 0.6194)
    No Information Rate : 0.5211          
    P-Value [Acc &gt; NIR] : &lt;2e-16          
                                          
                  Kappa : 0.2121          
                                          
 Mcnemar's Test P-Value : 0.4053          
                                          
            Sensitivity : 0.5967          
            Specificity : 0.6155          
         Pos Pred Value : 0.5879          
         Neg Pred Value : 0.6241          
             Prevalence : 0.4789          
         Detection Rate : 0.2858          
   Detection Prevalence : 0.4861          
      Balanced Accuracy : 0.6061          
                                          
       'Positive' Class : yes             
                                          </code></pre>
</div>
</div>
<p>En la salida tenemos algunas de las medidas de rendimiento estudiadas en el <a href="eval.html"><span>Capítulo&nbsp;6</span></a>. Sin embargo, no aparece el <span class="math inline">\(F_1-score\)</span>, media armónica de la Precisión y la Recuperación. En cualquier caso, es fácil de programar:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>clase.pred<span class="ot">=</span><span class="fu">ifelse</span>(predicciones<span class="sc">&gt;</span><span class="fl">0.5</span>,<span class="st">"yes"</span>,<span class="st">"no"</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>confmat1<span class="ot">=</span><span class="fu">confusionMatrix</span>(<span class="at">data=</span><span class="fu">as.factor</span>(clase.pred),<span class="at">reference=</span><span class="fu">as.factor</span>(bank.train<span class="sc">$</span>deposit))</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>Precision<span class="ot">=</span>confmat1<span class="sc">$</span>byClass[<span class="dv">3</span>]</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>Recall<span class="ot">=</span>confmat1<span class="sc">$</span>byClass[<span class="dv">1</span>]</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>F1score<span class="ot">=</span><span class="dv">2</span><span class="sc">*</span>Precision<span class="sc">*</span>Recall<span class="sc">/</span>(Precision<span class="sc">+</span>Recall)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(F1score)<span class="ot">=</span><span class="st">"F1Score"</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>F1score</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  F1Score 
0.6198061 </code></pre>
</div>
</div>
<p>Este es el funcionamiento del modelo en entrenamiento, pero ¿cuál es su funcionamiento en la partición de prueba? Recuerda, si el valor es mucho menor entonces estamos sobreajustando el modelo de entrenamiento.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>predicciones <span class="ot">&lt;-</span> <span class="fu">predict</span>(logit2, <span class="at">newdata=</span>bank.test, <span class="at">type=</span><span class="st">"response"</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>clase.pred<span class="ot">=</span><span class="fu">ifelse</span>(predicciones<span class="sc">&gt;</span><span class="fl">0.5</span>,<span class="st">"yes"</span>,<span class="st">"no"</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>confmat2<span class="ot">=</span><span class="fu">confusionMatrix</span>(<span class="at">data=</span><span class="fu">as.factor</span>(clase.pred),<span class="at">reference=</span><span class="fu">as.factor</span>(bank.test<span class="sc">$</span>deposit))</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>Precision<span class="ot">=</span>confmat2<span class="sc">$</span>byClass[<span class="dv">3</span>]</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>Recall<span class="ot">=</span>confmat2<span class="sc">$</span>byClass[<span class="dv">1</span>]</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>F1score<span class="ot">=</span><span class="dv">2</span><span class="sc">*</span>Precision<span class="sc">*</span>Recall<span class="sc">/</span>(Precision<span class="sc">+</span>Recall)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(F1score)<span class="ot">=</span><span class="st">"F1Score"</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>F1score</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  F1Score 
0.6137859 </code></pre>
</div>
</div>
<p>Vemos que el valor es muy similar al obtenido en entrenamiento. No estamos sobreajustando el modelo.</p>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tarea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Dejamos como labor del estudiante la tarea de mejorar la capacidad predictiva del modelo. Puedes emplear algunas de las variables que no han sido consideradas y que puedes consultar en el <a href="eda.html"><span>Capítulo&nbsp;3</span></a>.</p>
</div>
</div>
<p>Podemos calcular la curva <em>ROC</em> presentada en el <a href="eval.html"><span>Capítulo&nbsp;6</span></a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>roc_score<span class="ot">=</span><span class="fu">roc</span>(bank.test<span class="sc">$</span>deposit, predicciones,<span class="at">plot=</span><span class="cn">TRUE</span>,<span class="at">print.auc=</span><span class="cn">TRUE</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Y ahora, podemos plantear si este modelo es adecuado. Es decir ¿estamos satisfechos con el rendimiento del modelo? ¿Es suficientemente bueno?</p>
</section>
<section id="análisis-discriminante-lineal" class="level3" data-number="7.1.4">
<h3 data-number="7.1.4" class="anchored" data-anchor-id="análisis-discriminante-lineal"><span class="header-section-number">7.1.4</span> Análisis Discriminante Lineal</h3>
<p>El <em>análisis discriminante lineal (Linear Discriminant Analysis, LDA)</em> <span class="citation" data-cites="hastie2009elements">(<a href="references.html#ref-hastie2009elements" role="doc-biblioref">Hastie et&nbsp;al. 2009</a>)</span>, <span class="citation" data-cites="murphy2012machine">(<a href="references.html#ref-murphy2012machine" role="doc-biblioref">Murphy 2012</a>)</span> es una técnica estadística utilizada para encontrar una combinación lineal de variables que discrimine dos o más grupos.</p>
<p>Se trata de un algoritmo de <em>clasificación</em>, es decir, la variable objetivo es categórica. Sus dos supuestos principales son que las variables explicativas (que deben ser continuas) siguen una distribución Normal y que comparten varianza.</p>
<p>El <strong>objetivo</strong> de LDA es construir la combinación lineal de las variables explicativas que mejor discrimina las clases. En base a dicho hiperplano separador se clasificarán las nuevas observaciones.</p>
<p>Para explicar mejor cómo funciona este algoritmo, comencemos recordando el teorema de Bayes en una caso de clasificación, es decir, cuando tenemos una variable objetivo <span class="math inline">\(Y\)</span> y las variables explicativas <span class="math inline">\(\mathbf{X}\)</span>:</p>
<p><span class="math display">\[ P(C=1|\mathbf{X}=\mathbf{x})=\frac{f_{1}(\mathbf{x})P(1)}{\sum_{c=1}^{C_n}f_{c}(\mathbf{x})P(c)}\]</span></p>
<p>siendo <span class="math inline">\(P(C=1|\mathbf{X}=\mathbf{x})\)</span> la probabilidad a posteriori, es decir, la probabilidad de la clase <span class="math inline">\(1\)</span> dada la observación <span class="math inline">\(\mathbf{x}\)</span>. <span class="math inline">\(P(c)\)</span> es la probabilidad a priori de la clase <span class="math inline">\(c\)</span>. <span class="math inline">\(C_n\)</span> es el número total de clases. Nótese que <span class="math inline">\(\sum_{c=1}^{C_n}P(c) =1\)</span>. Finalmente <span class="math inline">\(f_c(\mathbf{x})\)</span> es la función de densidad condicional de las <span class="math inline">\(\mathbf{X}\)</span> en la clase <span class="math inline">\(c\)</span>. En el caso del LDA se asume que la densidad de cada clase es una Normal multivariante <span class="math display">\[
    f_c(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}e^{-\frac{1}{2}(\mathbf{x}-\mu_k)^{t}\Sigma_k^{-1}(\mathbf{x}-\mu_k)}
    \]</span></p>
<p>Además, también se asume que las clases comparten la misma matriz de varianzas covarianzas <span class="math inline">\(\Sigma_c=\Sigma, \ \forall c\)</span>.</p>
<p>En base a estas asunciones, comparemos las probabilidades de ambas clases:</p>
<p><span class="math display">\[
log\left( \frac{P(C=1|\mathbf{X}=\mathbf{x})}{P(C=0|\mathbf{X}=\mathbf{x})} \right) = log\left( \frac{f_1(\mathbf{x})}{f_0(\mathbf{x})} \right) + log\left( \frac{P(1)}{P(0)} \right) =\]</span></p>
<p><span class="math display">\[ = log\left( \frac{P(1)}{P(0)} \right) - \frac{1}{2}(\mu_1 + \mu_0)^{t}\Sigma^{-1}(\mu_1-\mu_0) + \mathbf{x}^{t}\Sigma^{-1}(\mu_1-\mu_0) \]</span></p>
<p>Obtenemos una ecuación que es lineal en <span class="math inline">\(\mathbf{x}\)</span>. Es decir, la frontera de decisión entre las clases <span class="math inline">\(0\)</span> y <span class="math inline">\(1\)</span> es una ecuación lineal, un hiperplano en dimensión <span class="math inline">\(p\)</span>.</p>
<p>En la práctica, se desconocen los parámetros de la distribución Normal. Se estiman con los datos de entrenamiento:</p>
<ul>
<li><p><span class="math inline">\(\widehat{P(c)} = n_c/n\)</span>, siendo <span class="math inline">\(n_c\)</span> el tamaño de la clase <span class="math inline">\(c\)</span> y <span class="math inline">\(n\)</span> el total</p></li>
<li><p><span class="math inline">\(\widehat{\mu}_c\)</span> media muestral de los elementos de la clase <span class="math inline">\(c\)</span></p></li>
<li><p><span class="math inline">\(\widehat{\Sigma}\)</span> varianza muestral de los elementos de la clase <span class="math inline">\(c\)</span></p></li>
</ul>
<p>¿Cómo se lleva entonces a cabo la clasificación? Como la frontera es un hiperplano separador, se comparará a qué población está más cercano cada observación. Esto es, para clasificar una observación como perteneciente a la clase <span class="math inline">\(1\)</span>, tendrá que ocurrir que</p>
<p><span class="math display">\[ log\left( \frac{P(C=1|\mathbf{X}=\mathbf{x})}{P(C=0|\mathbf{X}=\mathbf{x})} \right) &gt;0\]</span></p>
<p>y esto es equivalente a</p>
<p><span class="math display">\[ \mathbf{x}^{t} \widehat{\Sigma}^{-1}(\widehat{\mu}_1 - \widehat{\mu}_0) &gt; \frac{1}{2}(\widehat{\mu}_1 + \widehat{\mu}_0)^{t}\widehat{\Sigma}^{-1}(\widehat{\mu}_1 - \widehat{\mu}_0) - log(n_1/n_0) \]</span></p>
<p>Si, en esta ecuación, llamamos <span class="math inline">\(\mathbf{w}=\widehat{\Sigma}^{-1}(\widehat{\mu}_1 - \widehat{\mu}_0)\)</span>, podemos reescribir la expresión anterior anterior como</p>
<p><span class="math display">\[ \mathbf{x}^{t} \mathbf{w} &gt; \frac{1}{2}(\widehat{\mu}_1 + \widehat{\mu}_0)^{t}\mathbf{w} - log(n_1/n_0) \]</span> La frontera de decisión entre ambas clases es</p>
<p><span class="math display">\[ \mathbf{x}^{t} \mathbf{w} = \frac{1}{2}(\widehat{\mu}_1 + \widehat{\mu}_0)^{t}\mathbf{w} - log(n_1/n_0) \]</span> que es una combinación lineal de las variables explicativas.</p>
<p>¿Cómo se interpreta este procedimiento? Se está proyectando la observación a clasificar y las medias de las clases en una recta y se asigna la observación a la clase cuya media sea la más cercana.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Ventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body" title="Ventajas">
<ul>
<li><p><strong>Simple y rápido</strong></p></li>
<li><p><strong>Eficiente cuando se cumple las hipótesis</strong></p></li>
<li><p><strong>Clasificación de observaciones en grupos determinados</strong>. LDA se utiliza comúnmente para clasificar observaciones en grupos predeterminados, lo que facilita la interpretación y la toma de decisiones.</p></li>
<li><p><strong>Combinación de información para la frontera de decisión</strong>. Al considerar la información combinada de varias variables, LDA puede capturar patrones que podrían no ser evidentes al analizar cada variable por separado</p></li>
</ul>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Desventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body" title="Desventajas">
<ul>
<li><p><strong>Asume normalidad y homocedasticidad</strong>. LDA puede ser sensible a la falta de normalidad en las variables y a la diferente variabilidad en las mismas.</p></li>
<li><p><strong>Sensible a outliers</strong>. LDA puede ser sensible a la presencia de valores atípicos en los datos, lo que puede afectar negativamente la calidad del modelo.</p></li>
<li><p><strong>Es un clasificador lineal</strong>.Como su nombre indica, LDA es lineal y puede no funcionar bien en situaciones donde la relación entre las variables predictoras y la variable dependiente es no lineal.</p></li>
<li><p><strong>Requiere cierto tamaño de muestra</strong>. Para obtener resultados confiables, LDA requiere un tamaño de muestra adecuado en cada grupo, y puede no funcionar bien con tamaños de muestra desequilibrados</p></li>
</ul>
</div>
</div>
</div>
</section>
</section>
<section id="k-vecinos" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="k-vecinos"><span class="header-section-number">7.2</span> k-Vecinos</h2>
<p>El método de los <span class="math inline">\(k\)</span> vecinos más cercanos (<span class="math inline">\(k-NN\)</span>), abreviatura de “k nearest neighbors” en inglés, se cuenta entre los enfoques más simples y ampliamente utilizados en el campo del ML. Este método se basa en la noción de similitud (o distancia) entre observaciones.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Para recordar
</div>
</div>
<div class="callout-body-container callout-body">
<p>La principal asunción del modelo de los <span class="math inline">\(k\)</span> vecinos más cercanos es la existencia de un espacio de características en el cual observaciones similares se encuentran en proximidad.</p>
</div>
</div>
<p>A pesar de su simplicidad aparente, esta suposición plantea desafíos importantes. En primer lugar podemos plantearnos la elección adecuada del espacio de características, es decir, en la correcta selección de la métrica que definirá la similitud. No existe una fórmula mágica para determinar, a partir de un conjunto de datos dado, cuál es la métrica óptima a utilizar.</p>
<p>El segundo desafío se relaciona con la noción de “<em>cercanía</em>”. ¿Cómo definimos la cercanía entre dos observaciones? ¿Cuál es el volumen máximo del espacio dentro del cual consideramos que dos observaciones están cerca, y más allá del cual las consideramos distantes?</p>
<p>A pesar de estas cuestiones aparentemente simples, el método de los <span class="math inline">\(k\)</span> vecinos más cercanos se ha demostrado efectivo en una amplia variedad de aplicaciones, tanto en clasificación como en regresión. En este apartado, exploraremos cómo funciona este algoritmo, cómo ajustar sus parámetros y cómo aplicarlo de manera efectiva en situaciones del mundo real. A pesar de su simplicidad conceptual, los <span class="math inline">\(k\)</span> vecinos más cercanos siguen siendo una herramienta valiosa en el repertorio de ML.</p>
<p>El algoritmo de los <span class="math inline">\(k\)</span> vecinos más cercanos es un sencillo método de clasificación y regresión. En primer lugar, se eligen los parámetros del modelo:</p>
<ul>
<li><p>la métrica,</p></li>
<li><p>el número de vecinos, <span class="math inline">\(k\)</span>.</p></li>
</ul>
<p>En los ejemplos siguientes, se podrá observar que es habitual realizar pruebas con diferentes configuraciones de parámetros para identificar cuáles son los más apropiados. El principio fundamental del algoritmo implica obtener los <span class="math inline">\(k\)</span> vecinos más cercanos para cada observación en la muestra de datos. En un problema de clasificación, el algoritmo devolverá la clase que predomina entre los <span class="math inline">\(k\)</span> vecinos, es decir, la moda de la variable objetivo. En cambio, en un problema de regresión, el algoritmo proporcionará la media de la variable respuesta de los <span class="math inline">\(k\)</span> vecinos más cercanos.</p>
<p>Para determinar el valor óptimo de <span class="math inline">\(k\)</span> en función de los datos, ejecutamos el algoritmo <span class="math inline">\(k-NN\)</span> múltiples veces, utilizando distintos valores de <span class="math inline">\(k\)</span>, y seleccionamos aquel valor que minimice la tasa de errores, al mismo tiempo que preserva la capacidad del algoritmo para realizar predicciones precisas en datos no vistos previamente. Es importante destacar que valores muy bajos de <span class="math inline">\(k\)</span> pueden generar inestabilidad en el algoritmo, mientras que valores extremadamente altos de <span class="math inline">\(k\)</span> pueden aumentar el error en las predicciones. En este proceso, se busca el equilibrio adecuado que optimice el rendimiento del algoritmo en cada situación.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Ventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body" title="Ventajas">
<ul>
<li><p><strong>Sencillez y Facilidad de Implementación</strong>: Es uno de los algoritmos más sencillos y fáciles de entender en ML. No requiere suposiciones complejas o entrenamientos prolongados.</p></li>
<li><p><strong>Versatilidad</strong>: Se puede aplicar tanto a problemas de clasificación como de regresión.</p></li>
<li><p><strong>No Paramétrico</strong>: A diferencia de algunos algoritmos que asumen una distribución específica de los datos, <span class="math inline">\(k-NN\)</span> no hace suposiciones sobre la forma de los datos, lo que lo hace adecuado para una amplia gama de situaciones.</p></li>
<li><p><strong>Adaptabilidad a Datos Cambiantes</strong>: Puede adaptarse a cambios en los datos sin necesidad de volver a entrenar el modelo por completo, lo que lo hace útil en escenarios de datos en constante evolución. Ver <a href="nuevas.html"><span>Capítulo&nbsp;9</span></a>.</p></li>
<li><p><strong>Interpretabilidad</strong>: Las predicciones de k-NN se basan en la observación de datos cercanos, lo que facilita la interpretación de las decisiones del modelo.</p></li>
<li><p><strong>Robustez frente al ruido</strong>: Puede manejar datos con ruido, que no se adaptan a la distribución de la mayoría de los datos, sin un gran impacto en su rendimiento.</p></li>
<li><p><strong>Facilidad para Multiclases</strong>: Es aplicable a problemas de clasificación con múltiples clases sin necesidad de modificaciones significativas.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Desventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body" title="Desventajas">
<ul>
<li><p><strong>Sensibilidad a la elección de</strong> <span class="math inline">\(k\)</span>: La elección del parámetro que determina el número de vecinos puede ser crítica. Un valor demasiado pequeño puede hacer que el modelo sea sensible al ruido en los datos y cause sobreajuste, mientras que un valor demasiado grande puede hacer que el modelo sea demasiado suave y cause subajuste.</p></li>
<li><p><strong>Sensibilidad a la Elección de la Métrica de Distancia</strong>: La elección de la métrica de distancia es crucial y puede tener un impacto significativo en el rendimiento del algoritmo. No existe una métrica única que funcione para todos los problemas, y la elección de la métrica adecuada puede ser un desafío. Es muy probable que el científico de datos deba programar la métrica más adecuada para cada caso particular.</p></li>
<li><p><strong>Coste computacional</strong>: En conjuntos de datos grandes, calcular las distancias entre una nueva observación y todos los puntos de datos en el conjunto de entrenamiento puede ser computacionalmente costoso y lento. Esto limita su eficiencia en aplicaciones con grandes volúmenes de datos.</p></li>
<li><p><strong>Incapacidad para Capturar Relaciones Complejas</strong>: <span class="math inline">\(k-NN\)</span> es un algoritmo simple que no puede capturar relaciones complejas entre las características, como lo harían modelos más avanzados. No es adecuado para problemas con patrones no lineales.</p></li>
<li><p><strong>Problemas en Datos Desbalanceados</strong>: En problemas de clasificación con clases desequilibradas, donde una clase tiene muchas más muestras que otra, <span class="math inline">\(k-NN\)</span> tiende a sesgarse hacia la clase mayoritaria y puede no detectar bien la clase minoritaria.</p></li>
</ul>
</div>
</div>
</div>
<p>Aplicamos el algoritmo a nuestro problema de los datos bancarios.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Usamos 10-fold cross validation </span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>trainControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">"repeatedcv"</span>, <span class="at">number=</span><span class="dv">10</span>, <span class="at">repeats=</span><span class="dv">3</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>metric <span class="ot">&lt;-</span> <span class="st">"Accuracy"</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">128</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> bank.train <span class="sc">%&gt;%</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(housing,marital,education,balance,deposit)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>df[,<span class="dv">4</span>] <span class="ot">=</span> <span class="fu">scale</span>(df[,<span class="dv">4</span>])</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>fit.knn <span class="ot">&lt;-</span> <span class="fu">train</span>(deposit<span class="sc">~</span>., <span class="at">data=</span>df, <span class="at">method=</span><span class="st">"knn"</span>,</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">metric=</span>metric ,<span class="at">trControl=</span>trainControl)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>knn.k1 <span class="ot">&lt;-</span> fit.knn<span class="sc">$</span>bestTune </span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit.knn)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>k-Nearest Neighbors 

5581 samples
   4 predictor
   2 classes: 'no', 'yes' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 3 times) 
Summary of sample sizes: 5023, 5023, 5023, 5022, 5023, 5023, ... 
Resampling results across tuning parameters:

  k  Accuracy   Kappa    
  5  0.5842447  0.1655072
  7  0.5897985  0.1765086
  9  0.5930238  0.1835312

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was k = 9.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.knn)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Según el gráfico anterior, podríamos elegir el mejor valor de <span class="math inline">\(k\)</span>, si bien puede observarse pocas diferencias entre las diferentes opciones.</p>
<p>A continuación, obtenemos la predicción para el conjunto de datos de prueba e imprimimos la matriz de confusión.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">128</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>media<span class="ot">=</span><span class="fu">mean</span>(bank.train<span class="sc">$</span>balance)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>stddev<span class="ot">=</span><span class="fu">sqrt</span>(<span class="fu">var</span>(bank.train<span class="sc">$</span>balance))</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> bank.test <span class="sc">%&gt;%</span><span class="fu">mutate</span>(<span class="at">balance=</span>(balance<span class="sc">-</span>media)<span class="sc">/</span>stddev)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>prediction <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.knn,<span class="at">newdata=</span>df.test)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>cf <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(prediction, <span class="fu">as.factor</span>(df.test<span class="sc">$</span>deposit),<span class="at">positive=</span><span class="st">"yes"</span>)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  no yes
       no  900 578
       yes 568 744
                                          
               Accuracy : 0.5892          
                 95% CI : (0.5707, 0.6076)
    No Information Rate : 0.5262          
    P-Value [Acc &gt; NIR] : 1.216e-11       
                                          
                  Kappa : 0.1759          
                                          
 Mcnemar's Test P-Value : 0.7903          
                                          
            Sensitivity : 0.5628          
            Specificity : 0.6131          
         Pos Pred Value : 0.5671          
         Neg Pred Value : 0.6089          
             Prevalence : 0.4738          
         Detection Rate : 0.2667          
   Detection Prevalence : 0.4703          
      Balanced Accuracy : 0.5879          
                                          
       'Positive' Class : yes             
                                          </code></pre>
</div>
</div>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tarea
</div>
</div>
<div class="callout-body-container callout-body">
<p>En el código anterior, ¿Porqué se calcula la media y la varianza de la variable <code>balance</code>?</p>
</div>
</div>
</section>
<section id="grid-search" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="grid-search"><span class="header-section-number">7.3</span> Grid search</h2>
<p>La “<em>grid search</em>” (búsqueda en cuadrícula) es una técnica comúnmente utilizada en ML para encontrar los <strong>mejores hiperparámetros</strong> para un modelo. Tal y como vimos en el <a href="aprnosup.html"><span>Capítulo&nbsp;5</span></a>, los hiperparámetros son configuraciones que no se aprenden del conjunto de datos, sino que se establecen antes de entrenar el modelo. Estos hiperparámetros pueden incluir aspectos como la tasa de aprendizaje, la profundidad máxima de un árbol de decisión, el número de vecinos en el algoritmo de <span class="math inline">\(k-NN\)</span> y otros ajustes que afectan cómo se entrena y se ajusta el modelo.</p>
<p>La <em>grid search</em> consiste en definir una “<em>cuadrícula</em>” de posibles valores para los hiperparámetros que se desean optimizar. Por ejemplo, si se está trabajando con un modelo de máquinas de vectores de soporte (SVM), se podría tener una cuadrícula para los hiperparámetros de la función kernel y el parámetro de regularización. Luego, se entrena y evalúa el modelo utilizando todas las combinaciones posibles de valores en la cuadrícula.</p>
<p>El objetivo es encontrar la combinación de hiperparámetros que produce el mejor rendimiento del modelo según una métrica específica, como precisión, exactitud, <span class="math inline">\(F1\)</span>-score, error cuadrático medio, etc.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Para recordar
</div>
</div>
<div class="callout-body-container callout-body">
<p>La búsqueda en cuadrícula es un enfoque sistemático y automatizado que ahorra tiempo en comparación con probar manualmente diferentes combinaciones de hiperparámetros.</p>
</div>
</div>
<p>Si se tiene una cuadrícula de búsqueda para dos hiperparámetros, cada uno con tres valores posibles, se probarían un total de nueve combinaciones diferentes (3x3). El proceso de <em>grid search</em> evaluaría el rendimiento del modelo en nueve configuraciones distintas y seleccionaría aquella que obtiene los mejores resultados según la métrica definida.</p>
<p>Es importante mencionar exiten otras técnicas para la selección de hiperparámetros. Otros métodos, como la búsqueda aleatoria (“<em>random search</em>”), la optimización bayesiana y técnicas más avanzadas, también se utilizan en la selección de hiperparámetros en función de la complejidad y los recursos (computacionales, personales, temporales, etc) disponibles para la tarea.</p>
<p>En el ejemplo que nos ocupa, buscamos el mejor valor del parámetro <span class="math inline">\(k\)</span> entre <span class="math inline">\(1\)</span> y <span class="math inline">\(30\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1271</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">.k=</span><span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">30</span>,<span class="at">by=</span><span class="dv">1</span>))</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>fit.knn <span class="ot">&lt;-</span> <span class="fu">train</span>(deposit<span class="sc">~</span>., <span class="at">data=</span>df, <span class="at">method=</span><span class="st">"knn"</span>,</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">metric=</span>metric, <span class="at">tuneGrid=</span>grid, <span class="at">trControl=</span>trainControl)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>knn.k2 <span class="ot">&lt;-</span> fit.knn<span class="sc">$</span>bestTune </span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit.knn)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>k-Nearest Neighbors 

5581 samples
   4 predictor
   2 classes: 'no', 'yes' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 3 times) 
Summary of sample sizes: 5023, 5024, 5024, 5023, 5023, 5023, ... 
Resampling results across tuning parameters:

  k   Accuracy   Kappa    
   1  0.5941624  0.1869556
   2  0.5825199  0.1634009
   3  0.5840047  0.1656685
   4  0.5752866  0.1478903
   5  0.5826935  0.1625400
   6  0.5879512  0.1727362
   7  0.5917138  0.1802904
   8  0.5892608  0.1756808
   9  0.5887821  0.1747647
  10  0.5908722  0.1789482
  11  0.5936836  0.1845274
  12  0.5954742  0.1880368
  13  0.5968459  0.1908545
  14  0.6000722  0.1972805
  15  0.6013864  0.1999850
  16  0.6019835  0.2010479
  17  0.6048438  0.2067664
  18  0.6042492  0.2055601
  19  0.6067594  0.2104826
  20  0.6039522  0.2049647
  21  0.6056218  0.2082912
  22  0.6036533  0.2040438
  23  0.6053225  0.2074437
  24  0.6084891  0.2139799
  25  0.6072912  0.2116389
  26  0.6052587  0.2074789
  27  0.6072328  0.2115650
  28  0.6041256  0.2050824
  29  0.6069315  0.2107200
  30  0.6060367  0.2089063

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was k = 24.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.knn)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Encontramos óptimo <span class="math inline">\(k= 24\)</span>, el número de instancias más cercanas que hay que recopilar para hacer una predicción óptima. Ahora, usamos el modelo ajustado para predecir la clase para nuestro conjunto de prueba, e imprimir la matriz de confusión:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">128</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>prediction.knn <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.knn,<span class="at">newdata=</span>df.test,<span class="at">type=</span><span class="st">"prob"</span>)[,<span class="dv">2</span>]</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>clase.pred.knn<span class="ot">=</span><span class="fu">ifelse</span>(prediction.knn<span class="sc">&gt;</span><span class="fl">0.5</span>,<span class="st">"yes"</span>,<span class="st">"no"</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>cf <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(<span class="fu">as.factor</span>(clase.pred.knn), <span class="fu">as.factor</span>(df.test<span class="sc">$</span>deposit),<span class="at">positive=</span><span class="st">"yes"</span>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction   no  yes
       no  1028  603
       yes  440  719
                                          
               Accuracy : 0.6262          
                 95% CI : (0.6079, 0.6442)
    No Information Rate : 0.5262          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.2457          
                                          
 Mcnemar's Test P-Value : 5.271e-07       
                                          
            Sensitivity : 0.5439          
            Specificity : 0.7003          
         Pos Pred Value : 0.6204          
         Neg Pred Value : 0.6303          
             Prevalence : 0.4738          
         Detection Rate : 0.2577          
   Detection Prevalence : 0.4154          
      Balanced Accuracy : 0.6221          
                                          
       'Positive' Class : yes             
                                          </code></pre>
</div>
</div>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Métrica de disimilaridad
</div>
</div>
<div class="callout-body-container callout-body">
<p>Hasta ahora hemos empleado la distancia Euclídea, pero ¿es lo más adecuado en este caso? Averigua cómo modificar este hiperparámetro del modelo y estudia qué consecuencias tiene.</p>
</div>
</div>
</section>
<section id="árboles-de-decisión" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="árboles-de-decisión"><span class="header-section-number">7.4</span> Árboles de Decisión</h2>
<p>Los Árboles de Decisión (DT, de <em>Decision</em> <em>Tree</em> en inglés) son algoritmos de ML basados en la <strong>información</strong>. Este tipo de algoritmos determinan qué variables explicativas proporcionan la mayor parte de la información (<strong>ganancia de información</strong>) para medir la variable objetivo y hacen predicciones probando secuencialmente las características en orden a su información <span class="citation" data-cites="kelleher2018data">(<a href="references.html#ref-kelleher2018data" role="doc-biblioref">Kelleher y Tierney 2018</a>)</span>.</p>
<p>Un DT se crea mediante un proceso de particionamiento recursivo en el conjunto de variables explicativas, es decir, probando el valor de una característica y creando una rama del árbol para cada uno de sus posibles valores. Para una mejor comprensión de la estructura de un árbol, necesitamos definir los siguientes conceptos:</p>
<ul>
<li><strong>Nodo raíz</strong>: es el nodo origen, inicialmente todas las observaciones forman parte de este nodo.</li>
<li><strong>Nodos internos</strong>: son los nodos que se crean al definir reglas sobre una variable explicativa.</li>
<li><strong>Nodos hojas</strong>: son los nodos terminales del árbol.</li>
</ul>
<p>Habitualmente, favorecemos árboles que empleen el menor número de preguntas posible, lo que significa que buscamos árboles poco profundos. La construcción de árboles con poca profundidad implica que las características más informativas, es decir, aquellas que mejor distinguen entre observaciones con diferentes valores en la variable objetivo, deben ubicarse en la parte superior del árbol. Para llevar a cabo esta tarea, es fundamental contar con una métrica formal que evalúe cuán efectivamente una característica discrimina entre los niveles de la variable objetivo.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Entropía
</div>
</div>
<div class="callout-body-container callout-body">
<p>La <strong>entropía</strong> es una medida teórica de la “<em>incertidumbre</em>” contenida en un conjunto de datos, debido a la presencia de más de una posibilidad de clasificación.</p>
</div>
</div>
<p>La entropía es una medida de la impureza (heterogeneidad), de los datos. Desde un punto de vista formal, la entropía de un conjunto de <span class="math inline">\(N\)</span> valores distintos que son igualmente probables es el menor número de preguntas de tipo <em>sí/no</em> necesarias para determinar un valor desconocido extraído de las <span class="math inline">\(N\)</span> posibilidades:</p>
<p><span class="math display">\[Entropía=-\sum_{i=1}^N p_i log_2p_i\]</span></p>
<p>Podemos ver que la entropía, por definición, toma valores mayores o iguales a cero. Su valor máximo se alcanza cuando las observaciones se distribuyen equitativamente entre las <span class="math inline">\(N\)</span> posibles clases. La entropía toma su valor mínimo si, y sólo si, todas las observaciones están en la misma clase. Supongamos dos clases, esto es <span class="math inline">\(N=2\)</span>. En ese caso, consideramos la probabilidad de clase <span class="math inline">\(1\)</span> y la probabilidad de clase <span class="math inline">\(2\)</span> (<span class="math inline">\(p_1\)</span> y <span class="math inline">\(p_2\)</span> respectivamente). Si todas las observaciones están en la misma clase (por ejemplo, la clase <span class="math inline">\(1\)</span>), entonces: <span class="math inline">\(p1=1\)</span>, <span class="math inline">\(p2=0\)</span>. Y así:</p>
<p><span class="math display">\[Entropía=-(p_1log_2p_1+p_2log_2p_2)=-(1log_21+0log_20)=0\]</span>.</p>
<p>El algoritmo de DT basado en ganancia de la información sería como sigue:</p>
<ol type="1">
<li><p>Calcular la <strong>entropía</strong> del conjunto de datos original. ¿Cuánta información se requiere para organizar el conjunto de datos en conjuntos puros, donde todas las observaciones dentro del conjunto pertenecen a la misma clase? Esta medida nos da una idea de lo “complejo” que es nuestro problema.</p></li>
<li><p>Para cada variable explicativa, se crean los conjuntos resultantes <strong>dividiendo las observaciones</strong> en el conjunto de datos utilizando un umbral para dicha variable. A continuación, se suman los valores de entropía de cada uno de estos conjuntos. Calcular la información que sigue siendo necesaria para organizar las observaciones en conjuntos puros después de dividirlos utilizando la variable explicativa.</p></li>
<li><p>Calcular la <strong>Ganancia de Información</strong> restando la entropía restante (paso 2) del valor de entropía original (paso 1)</p></li>
</ol>
<p>Es decir, la idea es emplear los valores de las características medidas sobre las observaciones para dividir el conjunto inicial de datos en subconjuntos más pequeños repetidamente, hasta que la entropía de cada uno de los subconjuntos sea cero (o pequeña, menor que un umbral). Para cada partición, la entropía media de los subconjuntos resultantes debería ser menor que la del conjunto anterior.</p>
<p>En términos generales, la <strong>ganancia de información</strong> se define como la diferencia entre la entropía anterior y la nueva entropía. Por lo tanto, para cada nodo en el árbol, se elige para la división aquella variable que maximiza la ganancia de información.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Índice de Gini
</div>
</div>
<div class="callout-body-container callout-body">
<p>Existen otras métricas posibles para medir la calidad de la división. Por ejemplo, el <strong>índice de Gini</strong>:</p>
<p><span class="math display">\[Gini=-\sum_{i=1}^N p_i (1-p_i)\]</span></p>
</div>
</div>
<p>En el caso de árboles de regresión, donde la variable objetivo es cuantitativa, se define la reducción de la varianza como la disminución total de la varianza de la variable objetivo como resultado de la división del conjunto total en dos subconjuntos.</p>
<p>Existen varios criterios para detener la ejecución de este algoritmo de división. En un caso extremo podemos no detenerlo hasta que el error de clasificación sea <span class="math inline">\(0\)</span>. En dicho caso posiblemente cometamos errores de sobreajuste. Es decir, seleccionar divisiones sobre las variables hasta que todas las observaciones de la muestra de entrenamiento están perfectamente clasificadas funcionará bien, pero sólo sobre las observaciones de entrenamiento. Cuando ese mismo criterio se aplique a nuevas observaciones el resultado será, probablemente, mucho peor. En ocasiones se detiene el proceso de particionado cuando la profundidad del árbol supera un umbral (esto es, cuando se han empleado un número determinado de preguntas sobre las variables explicativas), o bien cuando el número de observaciones en el nodo a dividir es menor que un valor prefijado.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Para recordar
</div>
</div>
<div class="callout-body-container callout-body">
<p>La salida de un DT es un conjunto de reglas que se puede representar como un árbol donde cada nodo representa una decisión binaria.</p>
</div>
</div>
<p>A continuación aplicamos el método de DT al conjunto de datos <code>bank</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">128</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> bank.train <span class="sc">%&gt;%</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(age,job,housing,marital,education,duration,poutcome,balance,deposit)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>fit.dt <span class="ot">&lt;-</span> <span class="fu">rpart</span>(deposit<span class="sc">~</span>., <span class="at">data =</span> df, <span class="at">method =</span> <span class="st">'class'</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(fit.dt, <span class="at">extra =</span> <span class="dv">106</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Como podemos observar, la interpretación del modelo de DT es muy sencilla. Todas las observaciones están en el primer nodo (el nodo raíz). Sin realizar “preguntas” sobre las variables explicativas, el nodo es de clase “<em>no</em>” con una probabilidad de “<em>yes</em>” del <span class="math inline">\(0.48\)</span>.</p>
<p>La primera variable de interés es la <code>duration</code>:</p>
<ul>
<li><p>Si el valor es menor que <span class="math inline">\(343\)</span> (este valor es aprendido por el algoritmo y puede ser difícil de interpretar) entonces la observación va hacia la rama izquierda, cayendo en un nodo dónde la probabilidad de “yes” ha disminuido a <span class="math inline">\(0.30\)</span>. En ese nodo aparecen el <span class="math inline">\(62\%\)</span> de todas las observaciones del conjunto de entrenamiento.</p></li>
<li><p>Por contra, si el valor de <code>duration</code>es mayor que <span class="math inline">\(343\)</span> entonces la observación va hacia la rama derecha, cayendo en un nodo dónde la probabilidad de “yes” ha aumentado a <span class="math inline">\(0.77\)</span>. En ese nodo aparecen el <span class="math inline">\(38\%\)</span> de todas las observaciones del conjunto de entrenamiento. Además, fíjate que se trata de un nodo terminal.</p></li>
</ul>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tarea
</div>
</div>
<div class="callout-body-container callout-body">
<p>No se están teniendo en cuenta <em>todas</em> las variables disponibles en el conjunto de datos. El objetivo de la mayoría de los ejercicios de estos apuntes es mostrar a los alumnos cómo funcionan los métodos de ML. Si tu objetivo es obtener la más alta precisión, o el menor error, entonces te animamos a que intentes construir mejores modelos con, probablemente, más y mejores variables.</p>
</div>
</div>
<p>Podemos interpretar el resto de los nodos terminales del ejemplo como sigue:</p>
<table class="table">
<colgroup>
<col style="width: 4%">
<col style="width: 68%">
<col style="width: 16%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th>Nodo</th>
<th>Descripción</th>
<th style="text-align: left;">Tamaño relativo (%)</th>
<th style="text-align: left;">Prob(“yes”)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>duration &lt; 343, poutcome=failure,other or unkonwn, housing=yes</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">0.11</td>
</tr>
<tr class="even">
<td>2</td>
<td>duration &lt; 124, poutcome=failure,other or unkonwn, housing=yes</td>
<td style="text-align: left;">9.0</td>
<td style="text-align: left;">0.13</td>
</tr>
<tr class="odd">
<td>3</td>
<td>124&lt;=duration &lt; 343, poutcome=failure,other or unkonwn, housing=yes, balance&lt;274</td>
<td style="text-align: left;">7.0</td>
<td style="text-align: left;">0.29</td>
</tr>
<tr class="even">
<td>4</td>
<td>124&lt;=duration &lt; 343, poutcome=failure,other or unkonwn, housing=yes, balance&gt;=274</td>
<td style="text-align: left;">12.0</td>
<td style="text-align: left;">0.58</td>
</tr>
<tr class="odd">
<td>5</td>
<td>duration &lt; 343, poutcome=success</td>
<td style="text-align: left;">6.0</td>
<td style="text-align: left;">0.90</td>
</tr>
<tr class="even">
<td>6</td>
<td>duration&gt;343</td>
<td style="text-align: left;">38.0</td>
<td style="text-align: left;">0.77</td>
</tr>
</tbody>
</table>
<p>Usamos el modelo de DT para hacer predicciones:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sobre la partición de entrenamiento</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>prediction <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.dt, df, <span class="at">type =</span> <span class="st">'class'</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>cf <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(prediction, <span class="fu">as.factor</span>(df<span class="sc">$</span>deposit),<span class="at">positive=</span><span class="st">"yes"</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction   no  yes
       no  2090  349
       yes  818 2324
                                        
               Accuracy : 0.7909        
                 95% CI : (0.78, 0.8015)
    No Information Rate : 0.5211        
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16     
                                        
                  Kappa : 0.584         
                                        
 Mcnemar's Test P-Value : &lt; 2.2e-16     
                                        
            Sensitivity : 0.8694        
            Specificity : 0.7187        
         Pos Pred Value : 0.7397        
         Neg Pred Value : 0.8569        
             Prevalence : 0.4789        
         Detection Rate : 0.4164        
   Detection Prevalence : 0.5630        
      Balanced Accuracy : 0.7941        
                                        
       'Positive' Class : yes           
                                        </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sobre la partición de prueba</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>prediction.dt <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.dt, df.test, <span class="at">type =</span> <span class="st">'prob'</span>)[,<span class="dv">2</span>]</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>clase.pred<span class="ot">=</span><span class="fu">ifelse</span>(prediction.dt<span class="sc">&gt;</span><span class="fl">0.5</span>,<span class="st">"yes"</span>,<span class="st">"no"</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>cf <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(<span class="fu">as.factor</span>(clase.pred), <span class="fu">as.factor</span>(df.test<span class="sc">$</span>deposit),<span class="at">positive=</span><span class="st">"yes"</span>)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction   no  yes
       no  1178  381
       yes  290  941
                                          
               Accuracy : 0.7595          
                 95% CI : (0.7432, 0.7753)
    No Information Rate : 0.5262          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.516           
                                          
 Mcnemar's Test P-Value : 0.000512        
                                          
            Sensitivity : 0.7118          
            Specificity : 0.8025          
         Pos Pred Value : 0.7644          
         Neg Pred Value : 0.7556          
             Prevalence : 0.4738          
         Detection Rate : 0.3373          
   Detection Prevalence : 0.4412          
      Balanced Accuracy : 0.7571          
                                          
       'Positive' Class : yes             
                                          </code></pre>
</div>
</div>
<p>Al igual que en otros métodos de ML, podemos ajustar los hiperparámetros del modelo. En este caso ajustamos la máxima profundidad del árbol, el mínimo número de observaciones en un nodo para poder ser particionado en dos y el mínimo número de observaciones que un nodo terminal debe tener.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">4</span>,</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">minbucket =</span> <span class="fu">round</span>(<span class="dv">5</span> <span class="sc">/</span> <span class="dv">3</span>),</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">maxdepth =</span> <span class="dv">3</span>,</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">cp =</span> <span class="dv">0</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>tune.fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(deposit<span class="sc">~</span>., <span class="at">data =</span> df, <span class="at">method =</span> <span class="st">'class'</span>, <span class="at">control =</span> control)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tune.fit, <span class="at">extra =</span> <span class="dv">106</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>El nuevo modelo presenta un rendimiento ligeramente menor que el original pero respondiendo a condiciones que antes no imponíamos.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sobre la partición de prueba</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>prediction <span class="ot">&lt;-</span> <span class="fu">predict</span>(tune.fit, df.test, <span class="at">type =</span> <span class="st">'class'</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>cf <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(prediction, <span class="fu">as.factor</span>(df.test<span class="sc">$</span>deposit),<span class="at">positive=</span><span class="st">"yes"</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction   no  yes
       no  1186  393
       yes  282  929
                                          
               Accuracy : 0.7581          
                 95% CI : (0.7417, 0.7739)
    No Information Rate : 0.5262          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.5128          
                                          
 Mcnemar's Test P-Value : 2.297e-05       
                                          
            Sensitivity : 0.7027          
            Specificity : 0.8079          
         Pos Pred Value : 0.7671          
         Neg Pred Value : 0.7511          
             Prevalence : 0.4738          
         Detection Rate : 0.3330          
   Detection Prevalence : 0.4341          
      Balanced Accuracy : 0.7553          
                                          
       'Positive' Class : yes             
                                          </code></pre>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Ventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Explicabilidad</strong>: Los DT son altamente interpretables, lo que significa que sus resultados se pueden entender y comunicar de manera sencilla (ver <a href="nuevas.html"><span>Capítulo&nbsp;9</span></a>). Esto los hace ideales para tomar decisiones basadas en modelos en entornos donde se necesita entender el razonamiento detrás de las predicciones.</p></li>
<li><p><strong>Compatibilidad con Datos Mixtos</strong>: Pueden manejar tanto variables explicativas cuantitativas como cualitativas (categóricas). Esta versatilidad les permite trabajar con una amplia variedad de tipos de datos.</p></li>
<li><p><strong>Clasificación Multiclase</strong>: Los DT se pueden extender de manera natural para la clasificación multiclase, lo que significa que pueden asignar observaciones a más de dos categorías.</p></li>
<li><p><strong>Escalado de Variables no Requerido</strong>: A diferencia de algunos otros algoritmos, los DT no requieren que las variables se escalen o estandaricen antes de su uso. Esto facilita el proceso y ahorra tiempo en la preparación de datos.</p></li>
<li><p><strong>Manejo de Datos Faltantes</strong>: Los árboles pueden manejar datos faltantes sin necesidad de preprocesamiento adicional. Tratan los valores faltantes como una categoría adicional y no descartan observaciones con datos faltantes.</p></li>
<li><p><strong>Captura de No Linealidades</strong>: Los DT son capaces de captar relaciones no lineales entre variables, lo que es valioso cuando las relaciones entre las características y la variable objetivo son complejas y no se ajustan bien a modelos lineales.</p></li>
<li><p><strong>Robustez a Valores Atípicos</strong>: Los árboles son menos sensibles a valores atípicos en los datos en comparación con algunos otros algoritmos de aprendizaje automático. Los valores atípicos no tienen un impacto tan drástico en la construcción de árboles como en modelos lineales, por ejemplo.</p></li>
<li><p><strong>Eficiencia</strong>: Son relativamente eficientes en términos computacionales, especialmente para conjuntos de datos de tamaño moderado. La construcción de árboles puede realizarse de manera rápida.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Desventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Menos Precisión en Modelos Simples:</strong> Los DT a menudo son menos precisos que métodos más avanzados, como los bosques aleatorios o las redes neuronales, especialmente cuando se trata de problemas complejos. Su simplicidad a veces limita su capacidad para modelar relaciones sofisticadas en los datos.</p></li>
<li><p><strong>Sensibilidad a Cambios en los Datos:</strong> Pequeños cambios en los datos de entrenamiento pueden resultar en DT significativamente diferentes. Esto significa que los modelos basados en árboles son menos estables y robustos en comparación con algunos otros algoritmos. Un cambio en una observación o característica puede dar como resultado un árbol completamente diferente, lo que dificulta la confianza en las predicciones.</p></li>
<li><p><strong>Propensión al Sobreajuste:</strong> Los DT tienden a sobreajustarse a los datos de entrenamiento cuando se construyen demasiado profundos. Esto significa que el modelo se adapta demasiado a los datos de entrenamiento y puede no generalizar bien a nuevos datos. La elección de la profundidad óptima del árbol es crítica para evitar el sobreajuste.</p></li>
<li><p><strong>Dificultad para Modelar Relaciones Lineales:</strong> A pesar de su capacidad para capturar relaciones no lineales, los DT no son ideales para modelar relaciones lineales en los datos. En tales casos, otros modelos, como la regresión lineal, pueden ser más apropiados.</p></li>
<li><p><strong>Limitación en la Predicción de Valores Continuos:</strong> Los DT son adecuados para la clasificación, pero no son la mejor opción para la regresión o la predicción de valores continuos. Pueden proporcionar predicciones discretas, lo que puede ser una limitación en aplicaciones donde se requieren estimaciones precisas de valores numéricos.</p></li>
<li><p><strong>Sensibilidad a Valores Atípicos:</strong> Aunque son menos sensibles a los valores atípicos que algunos otros algoritmos, los DT todavía pueden verse afectados por valores extremos en los datos, lo que puede influir en la construcción del árbol y, por lo tanto, en las predicciones.</p></li>
<li><p><strong>No Son Óptimos para Datos de Alta Dimensión:</strong> Para conjuntos de datos de alta dimensión, la construcción de árboles puede volverse compleja y computacionalmente costosa. En tales casos, es posible que otros algoritmos, como las máquinas de vectores soporte (SVM) o el aprendizaje profundo, sean más apropiados.</p></li>
<li><p><strong>Difícil Interpretación en Árboles Profundos:</strong> Si los árboles se construyen muy profundos, pueden volverse difíciles de interpretar y visualizar, lo que reduce su utilidad en aplicaciones que requieren explicaciones claras de las decisiones del modelo.</p></li>
</ul>
</div>
</div>
</div>
<p>Si bien los DTs son una herramienta valiosa, es importante considerar estas limitaciones al seleccionar la técnica de ML adecuada para un problema específico. Las desventajas mencionadas anteriormente son algunas de las razones por las que se han desarrollado métodos más avanzados, basados en ensamblado como los Bosques Aleatorios, para abordar algunas de estas limitaciones.</p>
</section>
<section id="métodos-de-ensamblado" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="métodos-de-ensamblado"><span class="header-section-number">7.5</span> Métodos de ensamblado</h2>
<p>Los métodos de ensamblado son una potente estrategia en el campo del ML que se basa en la idea de que la unión de múltiples modelos puede mejorar significativamente el rendimiento de predicción en comparación con un solo modelo.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
La unión hace la fuerza
</div>
</div>
<div class="callout-body-container callout-body">
<p>En lugar de depender de un solo algoritmo para tomar decisiones, los métodos de ensamblado combinan las predicciones de varios modelos para obtener resultados más precisos y robustos.</p>
</div>
</div>
<p>Esta técnica se asemeja a la <em>sabiduría colectiva</em>: al reunir a un grupo diverso de personas, cada una con su conjunto único de conocimientos y perspectivas, se puede tomar una decisión más sólida y precisa. Del mismo modo, los métodos de ensamblado combinan múltiples modelos, cada uno de los cuales puede sobresalir en diferentes aspectos del problema, para lograr una predicción más precisa y confiable.</p>
<p>Los métodos de ensamblado se han convertido en un componente esencial en la caja de herramientas de los científicos de datos y profesionales del ML. Estas técnicas utilizan una variedad de algoritmos base, como DT, regresión logística, máquinas de vectores soporte y más, y los combinan de manera inteligente para mejorar la capacidad de generalización del modelo. En este enfoque, se pueden distinguir dos categorías principales de métodos de ensamblado:</p>
<ul>
<li><p>el ensamblado de <strong>bagging</strong></p></li>
<li><p>el ensamblado de <strong>boosting</strong></p></li>
</ul>
<section id="bagging" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="bagging"><span class="header-section-number">7.5.1</span> Bagging</h3>
<p>El ensamblado de <strong>Bagging</strong>, (de “<em>Bootstrap Aggregating</em>” en inglés), es una técnica de ML diseñada para mejorar la precisión y la estabilidad de los modelos predictivos. Se basa en la idea de construir múltiples modelos similares y combinar sus predicciones para obtener un resultado final más robusto y generalizable.</p>
<p>Damos, a continuación, una explicación detallada de cómo funciona el ensamblado de Bagging:</p>
<ol type="1">
<li><p><strong>Bootstrap</strong>: El proceso comienza dividiendo el conjunto de datos original en múltiples subconjuntos llamados conjuntos de entrenamiento, utilizando un método llamado muestreo con reemplazo. Esto significa que, en cada conjunto de entrenamiento, algunas muestras se seleccionan más de una vez, mientras que otras pueden quedar fuera. Este proceso genera múltiples conjuntos de entrenamiento, cada uno ligeramente diferente del original.</p></li>
<li><p><strong>Modelo Base</strong>: Luego, se entrena un modelo base, como un DT, regresión logística o cualquier otro algoritmo, en cada uno de estos conjuntos de entrenamiento. Cada modelo base se entrena en datos diferentes debido al muestreo con reemplazo, lo que da como resultado una serie de modelos que pueden variar en pequeñas diferencias.</p></li>
<li><p><strong>Predicciones</strong>: Una vez que se han entrenado todos los modelos base, se utilizan para hacer predicciones individuales sobre un conjunto de datos de prueba.</p></li>
<li><p><strong>Combinación</strong>: Finalmente, las predicciones de todos los modelos base se combinan para obtener una predicción agregada. En problemas de clasificación, esto suele implicar votación, donde se elige la clase con más votos, y en problemas de regresión, se calcula un promedio de las predicciones.</p></li>
</ol>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Ventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Reducción de la varianza</strong>: Al crear múltiples conjuntos de entrenamiento y modelos base, el ensamblado de Bagging reduce la varianza de las predicciones, lo que hace que el modelo sea más robusto y menos propenso al sobreajuste.</p></li>
<li><p><strong>Mayor precisión</strong>: La combinación de múltiples modelos base suele resultar en una precisión general superior en comparación con un solo modelo.</p></li>
<li><p><strong>Estabilidad</strong>: Al construir modelos ligeramente diferentes a partir de diferentes subconjuntos de datos, el ensamblado de Bagging aumenta la estabilidad y la resistencia del modelo ante datos ruidosos o atípicos.</p></li>
<li><p><strong>Mayor generalización</strong>: Debido a su capacidad para reducir el sobreajuste, el ensamblado de Bagging es efectivo para problemas de alta dimensionalidad y datos con ruido.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Desventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Mayor complejidad computacional:</strong> Debido a la necesidad de entrenar múltiples modelos base, el Bagging puede requerir más recursos computacionales y tiempo de entrenamiento en comparación con un solo modelo. Esto es especialmente relevante cuando se trabaja con conjuntos de datos grandes o algoritmos de modelo base complejos.</p></li>
<li><p><strong>Menos interpretabilidad:</strong> La combinación de múltiples modelos base dificulta la interpretación del modelo global. A diferencia de modelos individuales, como DT simples, los modelos Bagging no proporcionan una descripción sencilla de cómo se llega a una decisión o predicción.</p></li>
<li><p><strong>No garantiza la mejora:</strong> Aunque el Bagging suele mejorar la precisión y reducir la varianza en comparación con un solo modelo base, no garantiza un mejor rendimiento en todos los casos. En algunos conjuntos de datos o problemas, el Bagging puede no proporcionar mejoras significativas y, en raras ocasiones, podría incluso empeorar el rendimiento.</p></li>
<li><p><strong>Menos efectivo con modelos base inestables:</strong> Si se utilizan modelos base que son inherentemente inestables o propensos al sobreajuste, el Bagging puede no ser tan efectivo en mejorar su rendimiento. En tales casos, otros métodos de ensamblado, como Boosting, podrían ser más apropiados.</p></li>
<li><p><strong>Limitaciones en problemas de desequilibrio de clases:</strong> En problemas de clasificación con desequilibrio de clases, el Bagging puede no abordar adecuadamente el desafío de predecir clases minoritarias. En tales situaciones, se requieren técnicas específicas, como el ajuste de pesos de clase, para abordar el desequilibrio.</p></li>
</ul>
</div>
</div>
</div>
<p>El algoritmo más conocido que utiliza Bagging es el “<strong>Random Forest</strong>”, que combina múltiples DTs para lograr un modelo predictivo altamente preciso.</p>
<section id="random-forest" class="level4" data-number="7.5.1.1">
<h4 data-number="7.5.1.1" class="anchored" data-anchor-id="random-forest"><span class="header-section-number">7.5.1.1</span> Random Forest</h4>
<p>Un Bosque Aleatorio es una potente técnica de ensamblado que utiliza un conjunto de DTs para mejorar la precisión de las predicciones. En lugar de confiar en un solo árbol, se construye un bosque compuesto por numerosos árboles individuales. La singularidad de un Bosque Aleatorio radica en cómo se crean y combinan estos árboles.</p>
<p>Cada árbol dentro del Bosque Aleatorio no se crea a partir de todo el conjunto de datos, sino que se entrena con un subconjunto aleatorio de variables y un conjunto de observaciones seleccionadas al azar. Este proceso de muestreo aleatorio introduce diversidad en la construcción de cada árbol, lo que ayuda a mitigar la tendencia de DT a sobreajustar los datos.</p>
<p>Cada árbol individual en el Bosque Aleatorio genera una predicción para la variable objetivo, pero no se espera que cada árbol sea altamente efectivo por sí solo. La fortaleza del método radica en la combinación de numerosos árboles, cada uno de los cuales opera en una región diferente del espacio de características. El Bosque Aleatorio se basa en una regla de decisión que cuenta los votos de cada árbol para determinar la predicción final. En teoría, un gran número de modelos relativamente no correlacionados que funcionan como un comité superarán a cualquier modelo individual.</p>
<p>Una ventaja fundamental del Bosque Aleatorio es que aprovecha la sensibilidad de DTs a los datos en los que se entrenan. Cada árbol individual se entrena con una muestra aleatoria del conjunto de datos, permitiendo la selección con reemplazo. Esto da como resultado árboles diferentes en el bosque, lo que mejora la generalización del modelo.</p>
<p>Aplicamos este nuevo método de ML a nuestro conjunto de datos de <code>bank</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(<span class="fu">as.factor</span>(deposit)<span class="sc">~</span>., <span class="at">data=</span>df, <span class="at">importance=</span><span class="cn">TRUE</span>,<span class="at">proximity=</span><span class="cn">TRUE</span>) </span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
 randomForest(formula = as.factor(deposit) ~ ., data = df, importance = TRUE,      proximity = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 19.08%
Confusion matrix:
      no  yes class.error
no  2352  556   0.1911967
yes  509 2164   0.1904227</code></pre>
</div>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sobre la partición de prueba</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> bank.test <span class="sc">%&gt;%</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">fmarital=</span><span class="fu">as.factor</span>(marital))<span class="sc">%&gt;%</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(age,job,housing,<span class="at">marital=</span>fmarital,education,duration,poutcome,balance,deposit)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>df.test<span class="sc">$</span>deposit<span class="ot">=</span><span class="fu">as.factor</span>(df.test<span class="sc">$</span>deposit)</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>prediction.rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, df.test,<span class="at">type=</span><span class="st">"prob"</span>)[,<span class="dv">2</span>]</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>clase.pred.rf<span class="ot">=</span><span class="fu">ifelse</span>(prediction.rf<span class="sc">&gt;</span><span class="fl">0.5</span>,<span class="st">"yes"</span>,<span class="st">"no"</span>)</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>cf <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(<span class="fu">as.factor</span>(clase.pred.rf), <span class="fu">as.factor</span>(df.test<span class="sc">$</span>deposit),<span class="at">positive=</span><span class="st">"yes"</span>)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(cf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction   no  yes
       no  1157  242
       yes  311 1080
                                          
               Accuracy : 0.8018          
                 95% CI : (0.7865, 0.8164)
    No Information Rate : 0.5262          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
                                          
                  Kappa : 0.6035          
                                          
 Mcnemar's Test P-Value : 0.003832        
                                          
            Sensitivity : 0.8169          
            Specificity : 0.7881          
         Pos Pred Value : 0.7764          
         Neg Pred Value : 0.8270          
             Prevalence : 0.4738          
         Detection Rate : 0.3871          
   Detection Prevalence : 0.4986          
      Balanced Accuracy : 0.8025          
                                          
       'Positive' Class : yes             
                                          </code></pre>
</div>
</div>
<p>Podemos averiguar la importancia de las variables en el modelo.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">importance</span>(rf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  no       yes MeanDecreaseAccuracy MeanDecreaseGini
age        31.438028  33.11264             45.60845        301.41991
job        10.430697  13.43887             18.13609        132.83654
housing    34.414522  40.78079             47.86956        111.65045
marital     1.996905  16.98895             15.37603         62.10728
education  14.223616  17.20866             22.88143         79.54597
duration  132.331218 155.25993            160.32238       1043.86600
poutcome   67.541227  59.89374             76.32751        207.81210
balance    15.950109  22.08729             27.35226        330.12680</code></pre>
</div>
</div>
<p>Las tres primeras columnas hacen referencia a la medida MeanDecreaseAccuracy. La disminución del accuracy se mide permutando los valores del target en las muestras OOB y calculando la disminución correspondiente. Esto se hace para cada árbol y con sus muestras OOB correspondientes para obtener la media y la desviación estándar. La segunda medida es la disminución total de impurezas de nodos a partir de la división en la variable, promediada en todos los árboles.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Este gráfico muestra la importancia de las variables, ordenadas de mayor a menor. En este caso particular la variable más relevante dentro del modelo es la duración de la llamada.</p>
<p>Además podemos obtener una visualización del funcionamiento del Bosque con la función <code>MDSplot</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co">#MDSplot(rf,as.factor(df$deposit),k=3)</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La función grafica las <span class="math inline">\(k\)</span> primeras componentes principales (ver <a href="reddim.html"><span>Capítulo&nbsp;4</span></a>) de la matriz de proximidad, que es una matriz de medidas de proximidad entre la observaciones de entrenamiento (basada en la frecuencia con que los pares de puntos de datos se encuentran en los mismos nodos terminales).</p>
</section>
</section>
<section id="boosting" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="boosting"><span class="header-section-number">7.5.2</span> Boosting</h3>
<p>El ensamblado de <strong>Boosting</strong> es una técnica de ML que se utiliza para mejorar la precisión de los modelos de clasificación o regresión. A diferencia del Bagging, que se basa en la construcción de múltiples modelos independientes y su promediación, el Boosting se centra en mejorar iterativamente un solo modelo débil (o sencillo).</p>
<p>El proceso de Boosting funciona de la siguiente manera:</p>
<ol type="1">
<li><p>Comienza con un <strong>modelo base</strong> (generalmente un modelo simple o débil), que se entrena en el conjunto de datos original.</p></li>
<li><p>Luego, se evalúa el rendimiento del modelo base. Las instancias clasificadas incorrectamente o <strong>las predicciones con errores se ponderan más fuertemente</strong> para la siguiente iteración.</p></li>
<li><p>En la siguiente iteración, se entrena un <strong>segundo modelo débil</strong>, pero esta vez se da más énfasis a las instancias que se clasificaron incorrectamente en la iteración anterior.</p></li>
<li><p>Los resultados de los modelos base se combinan ponderando sus predicciones en función de su <strong>rendimiento relativo</strong>. Los modelos que tienen un mejor rendimiento reciben más peso en la predicción final.</p></li>
<li><p>Este proceso de entrenamiento, evaluación y asignación de pesos se repite durante <strong>varias iteraciones</strong> (también llamadas etapas de Boosting). Cada nuevo modelo se enfoca en corregir las deficiencias del modelo anterior.</p></li>
<li><p>Al final de las iteraciones, se obtiene un <strong>modelo ensamblado fuerte</strong> que es capaz de mejorar significativamente la precisión de las predicciones en comparación con un solo modelo base.</p></li>
</ol>
<p>Algunos algoritmos de Boosting populares incluyen AdaBoost, Gradient Boosting, y XGBoost.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Ventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Mejora del rendimiento:</strong> El Boosting es conocido por aumentar significativamente la precisión de las predicciones en comparación con modelos individuales o modelos base.</p></li>
<li><p><strong>Reducción del sesgo:</strong> A través del proceso iterativo, el Boosting puede reducir el sesgo del modelo, lo que significa que se vuelve más capaz de ajustarse a los datos y hacer predicciones precisas.</p></li>
<li><p><strong>Manejo de datos desequilibrados:</strong> Es eficaz en la clasificación de conjuntos de datos con clases desequilibradas, ya que se enfoca en las instancias mal clasificadas.</p></li>
<li><p><strong>Adaptabilidad:</strong> Funciona bien con una variedad de algoritmos base, lo que brinda flexibilidad al elegir el modelo débil más adecuado.</p></li>
<li><p><strong>Versatilidad:</strong> El Boosting se utiliza en problemas de clasificación y regresión, lo que lo hace adecuado para una amplia gama de aplicaciones.</p></li>
<li><p><strong>Capacidad para detectar patrones complejos:</strong> Al mejorar iterativamente el modelo, el Boosting puede capturar patrones complejos en los datos y generar modelos más precisos.</p></li>
<li><p><strong>Robustez:</strong> Puede manejar ruido en los datos y es menos propenso al sobreajuste en comparación con algunos otros métodos de aprendizaje automático.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Desventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Sensibilidad al ruido:</strong> Los modelos de Boosting pueden ser sensibles al ruido y a valores atípicos en los datos. Si el conjunto de datos contiene observaciones erróneas o extremas, el Boosting puede sobreajustarse a estos valores inusuales.</p></li>
<li><p><strong>Tiempo de entrenamiento:</strong> El proceso de Boosting implica entrenar múltiples modelos base de manera secuencial, lo que puede llevar más tiempo en comparación con otros métodos de ensamblado más simples.</p></li>
<li><p><strong>Menos interpretabilidad:</strong> A medida que se agregan más modelos al ensamblado, la interpretabilidad del modelo general puede disminuir. Esto hace que sea más difícil comprender y explicar las predicciones del modelo.</p></li>
<li><p><strong>Necesidad de ajuste de hiperparámetros:</strong> Los modelos de Boosting tienen varios hiperparámetros que deben ajustarse adecuadamente para obtener el mejor rendimiento. Encontrar la combinación óptima de hiperparámetros puede requerir tiempo y recursos computacionales.</p></li>
<li><p><strong>Potencial sobreajuste:</strong> Si no se ajustan cuidadosamente los hiperparámetros, los modelos de Boosting pueden estar sujetos al sobreajuste, especialmente si el número de iteraciones (número de modelos base) es demasiado alto.</p></li>
<li><p><strong>Requisitos de recursos computacionales:</strong> Dependiendo de la implementación y la cantidad de modelos base, los modelos de Boosting pueden requerir recursos computacionales significativos, lo que podría ser un problema en sistemas con recursos limitados.</p></li>
</ul>
</div>
</div>
</div>
<section id="xgboost" class="level4" data-number="7.5.2.1">
<h4 data-number="7.5.2.1" class="anchored" data-anchor-id="xgboost"><span class="header-section-number">7.5.2.1</span> XGBoost</h4>
<p><strong>XGBoost</strong>, que significa “<em>Extreme Gradient Boosting</em>”, es un modelo de ML de tipo ensamblado que ha ganado gran popularidad en competiciones de ciencia de datos y aplicaciones prácticas debido a su alto rendimiento y eficacia en una variedad de problemas de clasificación y regresión. XGBoost es una poderosa técnica de ML que ha demostrado su eficacia en una amplia gama de aplicaciones. Su capacidad para manejar datos ruidosos, seleccionar características importantes y su velocidad de entrenamiento lo hacen valioso tanto para profesionales de datos como para científicos de datos en competiciones y proyectos del mundo real.</p>
<p>XGBoost es una extensión del método de <em>Gradient Boosting</em>, que se centra en mejorar las debilidades de dicho algoritmo. ¿Cómo funciona el método del Gradient Boosting?. Lo vemos en la siguiente figura:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="aprsup/grad1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">https://www.kaggle.com/code/rtatman/machine-learning-with-xgboost-in-r</figcaption><p></p>
</figure>
</div>
<p>Pasamos por ciclos que construyen repetidamente nuevos modelos y los combinan en un modelo de ensamblado. Empezamos el ciclo tomando un modelo existente y calculando los errores de cada observación del conjunto de datos. A continuación, construimos un nuevo modelo para predecir estos errores. Añadimos las predicciones de este modelo de predicción de errores al ensamblado de modelos.</p>
<p>Para hacer una predicción, sumamos las predicciones de todos los modelos anteriores. Podemos utilizar estas predicciones para calcular nuevos errores, construir el siguiente modelo y añadirlo al ensamblado.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(purrr)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(xgboost)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="co">#convertimos los datos a numéricos</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> <span class="fu">map_df</span>(df, <span class="cf">function</span>(columna) {</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>  columna <span class="sc">%&gt;%</span> </span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.factor</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>    as.numeric <span class="sc">%&gt;%</span> </span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>    { . <span class="sc">-</span> <span class="dv">1</span> }</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>datos <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>datos<span class="sc">$</span>train <span class="ot">&lt;-</span> df.train</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> <span class="fu">map_df</span>(df.test, <span class="cf">function</span>(columna) {</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>  columna <span class="sc">%&gt;%</span> </span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.factor</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>    as.numeric <span class="sc">%&gt;%</span> </span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>    { . <span class="sc">-</span> <span class="dv">1</span> }</span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>datos<span class="sc">$</span>test <span class="ot">&lt;-</span> df.test</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Convertimos los datos al formato Dmatrix</span></span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a>datos<span class="sc">$</span>train_mat  <span class="ot">&lt;-</span> </span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>  datos<span class="sc">$</span>train <span class="sc">%&gt;%</span> </span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>deposit) <span class="sc">%&gt;%</span> </span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xgb.DMatrix</span>(<span class="at">data =</span> ., <span class="at">label =</span> datos<span class="sc">$</span>train<span class="sc">$</span>deposit)</span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a>datos<span class="sc">$</span>test_mat  <span class="ot">&lt;-</span> </span>
<span id="cb62-34"><a href="#cb62-34" aria-hidden="true" tabindex="-1"></a>  datos<span class="sc">$</span>test <span class="sc">%&gt;%</span> </span>
<span id="cb62-35"><a href="#cb62-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>deposit) <span class="sc">%&gt;%</span> </span>
<span id="cb62-36"><a href="#cb62-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.matrix</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb62-37"><a href="#cb62-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xgb.DMatrix</span>(<span class="at">data =</span> ., <span class="at">label =</span> datos<span class="sc">$</span>test<span class="sc">$</span>deposit)</span>
<span id="cb62-38"><a href="#cb62-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-39"><a href="#cb62-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenamiento del modelo</span></span>
<span id="cb62-40"><a href="#cb62-40" aria-hidden="true" tabindex="-1"></a>datos<span class="sc">$</span>modelo_01 <span class="ot">&lt;-</span> <span class="fu">xgboost</span>(<span class="at">data =</span> datos<span class="sc">$</span>train_mat,  </span>
<span id="cb62-41"><a href="#cb62-41" aria-hidden="true" tabindex="-1"></a>                           <span class="at">objective =</span> <span class="st">"binary:logistic"</span>, <span class="co">#clasificación binaria</span></span>
<span id="cb62-42"><a href="#cb62-42" aria-hidden="true" tabindex="-1"></a>                           <span class="at">nround =</span> <span class="dv">10</span>, <span class="co"># número máximo de iteraciones boosting </span></span>
<span id="cb62-43"><a href="#cb62-43" aria-hidden="true" tabindex="-1"></a>                           <span class="at">max_depth=</span><span class="dv">2</span>, <span class="co"># número de nodos de bifurcación de los árboles de de decisión usados en el entrenamiento</span></span>
<span id="cb62-44"><a href="#cb62-44" aria-hidden="true" tabindex="-1"></a>                           <span class="at">eta =</span><span class="fl">0.3</span>, <span class="co"># La tasa de aprendizaje del modelo</span></span>
<span id="cb62-45"><a href="#cb62-45" aria-hidden="true" tabindex="-1"></a>                           <span class="at">nthread =</span><span class="dv">2</span>) <span class="co">#  El número de hilos computacionales que serán usados en el proceso de entrenamiento. </span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] train-logloss:0.620493 
[2] train-logloss:0.576189 
[3] train-logloss:0.547507 
[4] train-logloss:0.527672 
[5] train-logloss:0.507268 
[6] train-logloss:0.496584 
[7] train-logloss:0.485207 
[8] train-logloss:0.473366 
[9] train-logloss:0.467606 
[10]    train-logloss:0.462035 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>datos<span class="sc">$</span>modelo_01</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##### xgb.Booster
raw: 12.7 Kb 
call:
  xgb.train(params = params, data = dtrain, nrounds = nrounds, 
    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, 
    early_stopping_rounds = early_stopping_rounds, maximize = maximize, 
    save_period = save_period, save_name = save_name, xgb_model = xgb_model, 
    callbacks = callbacks, objective = "binary:logistic", max_depth = 2, 
    eta = 0.3, nthread = 2)
params (as set within xgb.train):
  objective = "binary:logistic", max_depth = "2", eta = "0.3", nthread = "2", validate_parameters = "TRUE"
xgb.attributes:
  niter
callbacks:
  cb.print.evaluation(period = print_every_n)
  cb.evaluation.log()
# of features: 8 
niter: 10
nfeatures : 8 
evaluation_log:
     iter train_logloss
    &lt;num&gt;         &lt;num&gt;
        1     0.6204931
        2     0.5761893
---                    
        9     0.4676060
       10     0.4620354</code></pre>
</div>
</div>
<p>Ahora podemos examinar e interpretar nuestro modelo XGBoost. Una forma en que podemos examinar nuestro modelo es observando una representación de la combinación de todos los DTs en nuestro modelo. Dado que todos los árboles tienen la misma profundidad podemos apilarlos unos encima de otros y elegir las características que aparecen con más frecuencia en cada nodo.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización del conjunto de árboles como una única unidad </span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="fu">xgb.plot.multi.trees</span>(<span class="at">model =</span> datos<span class="sc">$</span>modelo_01)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Column 2 ['No'] of item 2 is missing in item 1. Use fill=TRUE to fill with NA (NULL for list columns), or use.names=FALSE to ignore column names. use.names='check' (default from v1.12.2) emits this message and proceeds as if use.names=FALSE for  backwards compatibility. See news item 5 in v1.12.2 for options to control this message.</code></pre>
</div>
<div class="cell-output-display">
<div id="htmlwidget-26265056f2eb5ce25e74" style="width:100%;height:464px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-26265056f2eb5ce25e74">{"x":{"diagram":"digraph {\n\ngraph [layout = \"dot\",\n       rankdir = \"LR\"]\n\nnode [color = \"DimGray\",\n      fillcolor = \"beige\",\n      style = \"filled\",\n      shape = \"rectangle\",\n      fontname = \"Helvetica\"]\n\nedge [color = \"DimGray\",\n     arrowsize = \"1.5\",\n     arrowhead = \"vee\",\n     fontname = \"Helvetica\"]\n\n  \"1\" [label = \"duration (2984.53)\nhousing (  71.14)\nage (  55.56)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"2\" [label = \"poutcome (358.950)\nduration (112.584)\nhousing (318.900)\nage (129.145)\nbalance ( 79.631)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"3\" [label = \"duration (418.88)\nhousing (119.48)\npoutcome (299.20)\nbalance ( 68.00)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"4\" [label = \"Leaf (-1.2496)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"5\" [label = \"Leaf (-1.0535)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"6\" [label = \"Leaf (1.0899)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"7\" [label = \"Leaf (0.78719)\", fillcolor = \"#F5F5DC\", fontcolor = \"#000000\"] \n  \"1\"->\"2\" \n  \"2\"->\"4\" \n  \"3\"->\"6\" \n  \"1\"->\"3\" \n  \"2\"->\"5\" \n  \"3\"->\"7\" \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<p>En esta figura, la parte superior del árbol está a la izquierda y la parte inferior a la derecha. En el caso de las características, el número que aparece al lado es la “<em>calidad</em>”, que ayuda a indicar la importancia de la característica en todos los árboles. Una mayor calidad significa que la característica es más importante. Por tanto, podemos decir que <code>duration</code> es con diferencia la característica más importante en todos nuestros árboles, tanto porque está más arriba en el árbol como porque su puntuación de calidad es muy alta.</p>
<p>Para los nodos “<em>hoja</em>”, el número es el valor medio del modelo devuelto a través de todos los árboles cuando una determinada observación terminó en esa hoja. Como estamos utilizando un modelo logístico, nos devuelve diciendo que el logaritmo de probabilidades en lugar de la probabilidad. Sin embargo, podemos convertir fácilmente las probabilidades logarítmicas en probabilidad.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convertimos log odds a probabilidades</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>odds_to_probs <span class="ot">&lt;-</span> <span class="cf">function</span>(odds){</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">exp</span>(odds)<span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(odds)))</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ejemplo </span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="fu">odds_to_probs</span>(<span class="sc">-</span><span class="fl">1.2496</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2227694</code></pre>
</div>
</div>
<p>Así, en los árboles en los que una observación acabó en esa hoja, la probabilidad media de que una observación fuera un “<em>yes</em>” en la variable <code>deposit</code> es del <span class="math inline">\(22\%\)</span>.</p>
<p>¿Y si queremos ver rápidamente qué características son las más importantes? Podemos hacerlo creando, y luego trazando, la matriz de importancia, como sigue.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># importancia sobre cada característica</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>importance_matrix <span class="ot">&lt;-</span> <span class="fu">xgb.importance</span>(<span class="at">model =</span> datos<span class="sc">$</span>modelo_01)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="fu">xgb.plot.importance</span>(importance_matrix)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-33-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Pasamos a obtener las predicciones. El resultado es un vector de valores numéricos, cada uno representando la probabilidad de que un caso en particular pertenezca al valor <span class="math inline">\(1\)</span> de nuestra variable objetivo. Es decir, la probabilidad de que esa observación sea un “<em>yes</em>”.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones en la muestra de prueba</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>datos<span class="sc">$</span>predict_01 <span class="ot">&lt;-</span> <span class="fu">predict</span>(datos<span class="sc">$</span>modelo_01, datos<span class="sc">$</span>test_mat)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(datos<span class="sc">$</span>predict_01 <span class="sc">&gt;</span> <span class="fl">0.5</span>, datos<span class="sc">$</span>test<span class="sc">$</span>deposit) <span class="sc">%&gt;%</span> </span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">table</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">confusionMatrix</span>(<span class="at">positive=</span><span class="st">"1"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

   X2
X1     0    1
  0 1188  319
  1  280 1003
                                          
               Accuracy : 0.7853          
                 95% CI : (0.7696, 0.8004)
    No Information Rate : 0.5262          
    P-Value [Acc &gt; NIR] : &lt;2e-16          
                                          
                  Kappa : 0.5688          
                                          
 Mcnemar's Test P-Value : 0.1205          
                                          
            Sensitivity : 0.7587          
            Specificity : 0.8093          
         Pos Pred Value : 0.7818          
         Neg Pred Value : 0.7883          
             Prevalence : 0.4738          
         Detection Rate : 0.3595          
   Detection Prevalence : 0.4599          
      Balanced Accuracy : 0.7840          
                                          
       'Positive' Class : 1               
                                          </code></pre>
</div>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Time consuming
</div>
</div>
<div class="callout-body-container callout-body">
<p>La tarea que más tiempo consume al usar el modelo XGBoost es encontrar los mejores hiperparámetros para alcanzar la mayor precisión posible de un modelo.</p>
</div>
</div>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tarea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Queda como tarea del alumno modificar los hiperparámetros del modelo, buscando una mejora en su rendimiento</p>
</div>
</div>
</section>
</section>
</section>
<section id="naive-bayes" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="naive-bayes"><span class="header-section-number">7.6</span> Naive Bayes</h2>
<p>El clasificador ingenuo de Bayes (en inglés “<em>Naïve Bayes</em>”) es un clasificador sencillo que se basa en el conocido <strong>teorema de Bayes</strong>. A pesar de su simplicidad, los clasificadores “ingenuos” de Bayes son especialmente útiles para problemas con muchas variables de entrada, variables de entrada cuantitativas con un número muy grande de valores posibles y clasificación de texto. Es uno de los algoritmos básicos que suele aparecer en problemas de <em>Procesamiento de Lenguaje Natural</em>. Podrás estudiar aspectos fundamentales de Lenguaje Natural a lo largo de tus estudios de grado.</p>
<p>El teorema de Bayes relaciona la probabilidad condicional de dos eventos A y B. La idea es modificar nuestras creencias iniciales sobre lo que ha sucedido (a priori) proporcionalmente con la forma en que nuestras observaciones se relacionan con sus posibles causas (probabilidad inversa):</p>
<p><span class="math display">\[P(A∩B)=P(A,B)=P(A)*P(B|A)=P(B)*P(A|B)⇒P(B|A)=\frac{P(B)P(A|B)}{P(A)}
\]</span></p>
<p>Su aplicación a un problema de clasificación es como sigue. Supongamos <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> variables explicativas independientes dado el valor de la variable objetivo <span class="math inline">\(Y_k\)</span>. Es decir, suponemos:</p>
<p><span class="math display">\[P(X_1|X_2,\ldots,X_n,Y_k)=P(X_1|Y_k); P(X_2|X_3,\ldots,X_n,Y_k)=P(X_2|Y_k)\]</span>, etc.</p>
<p>De este modo, aplicando el teorema de Bayes de manera recursiva, se tiene:</p>
<p><span class="math display">\[P(X_1,X_2,\ldots,X_n,Y_k)=P(Y_k)P(X_n|Y_k)P(X_{n-1}|X_nY_k)\ldots P(X_1|X_2,\ldots,X_n,Y_k)=\]</span> <span class="math display">\[P(Y_k)P(X_n|Y_k)P(X_{n-1}|Y_k)\ldots P(X_1|Y_k)\]</span></p>
<p>Por tanto:</p>
<p><span class="math display">\[P(Y_k|X_1,\ldots,X_n)=\frac{P(Y_k)\prod_{j=1}^nP(X_j|Y_k)}{P(X_1,X_2,\ldots,X_n)}\]</span></p>
<p>Esta es la expresión de la probabilidad de un valor de la variable respuesta dado el conjunto de valores de las variables explicativas. El denominador es constante, de modo que bastará con considerar sólo el numerador para comparar las probabilidades de las diferentes clases condicionadas a los valores de las variables explicativas. Se evalúa el numerador para todos los valores de la variable objetivo y se obtienen las diferentes probabilidades de clase.</p>
<p>La probabilidad de clase <span class="math inline">\(P(Y_k)\)</span> se denomina probabilidad a priori, y:</p>
<p><span class="math display">\[\prod_{j=1}^nP(X_j|Y_k) \]</span></p>
<p>es la verosimilitud. La verosimilitud mide cómo de verosímiles son las observaciones de las variables explicativas dado que se ha observado una determinada etiqueta en la variable respuesta. Si la variable <span class="math inline">\(X_j\)</span> es cualitativa, para estimar la verosimilitud de la clase <span class="math inline">\(k\)</span> con el predictor <span class="math inline">\(j\)</span>, se cuenta la proporción de observaciones de la muestra de entrenamiento de la clase <span class="math inline">\(k\)</span> en cada categoría de dicha variable. Si la variable <span class="math inline">\(X_j\)</span> es cuantitativa, habitualmente se asume una distribución Normal (aunque también se pueden usar opciones no paramétricas como histogramas).</p>
<p>Finalmente se predice para cada conjunto de variables explicativas la clase con mayor probabilidad de clase condicionada <span class="math inline">\(P(Y_k|X_1,\ldots,X_n)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(naivebayes)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">naive_bayes</span>(<span class="fu">as.factor</span>(deposit) <span class="sc">~</span> ., <span class="at">data =</span> df.train, <span class="at">usekernel =</span> T) </span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>probabilities <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, df.test[,<span class="sc">-</span><span class="dv">9</span>],<span class="at">type=</span><span class="st">"prob"</span>)[,<span class="dv">2</span>]</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>classes <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(probabilities<span class="sc">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="fu">table</span>(classes, datos<span class="sc">$</span>test<span class="sc">$</span>deposit),<span class="at">positive=</span><span class="st">"1"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

       
classes    0    1
      0 1166  264
      1  302 1058
                                          
               Accuracy : 0.7971          
                 95% CI : (0.7817, 0.8119)
    No Information Rate : 0.5262          
    P-Value [Acc &gt; NIR] : &lt;2e-16          
                                          
                  Kappa : 0.5937          
                                          
 Mcnemar's Test P-Value : 0.1199          
                                          
            Sensitivity : 0.8003          
            Specificity : 0.7943          
         Pos Pred Value : 0.7779          
         Neg Pred Value : 0.8154          
             Prevalence : 0.4738          
         Detection Rate : 0.3792          
   Detection Prevalence : 0.4875          
      Balanced Accuracy : 0.7973          
                                          
       'Positive' Class : 1               
                                          </code></pre>
</div>
</div>
</section>
<section id="modelos-de-mezcla-de-gaussianas" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="modelos-de-mezcla-de-gaussianas"><span class="header-section-number">7.7</span> Modelos de mezcla de Gaussianas</h2>
<p>Los modelos de mezcla de gaussianas (<strong>GMM</strong>, por sus siglas en inglés, “<em>Gaussian Mixture Models</em>”) son una técnica de ML utilizada para modelar datos con estructuras complejas y distribuciones múltiples. Estos modelos se basan en la idea de que un conjunto de datos puede estar compuesto por varias distribuciones gaussianas (también conocidas como normales). En otras palabras, los GMM permiten descomponer un conjunto de datos en múltiples componentes gaussianas, cada una de las cuales describe una parte de la distribución de los datos.</p>
<p>Los datos en el mundo real rara vez siguen una distribución simple y unimodal (¡ninguna teoría es tan compleja como un conjunto de datos!). Los GMM abordan esta complejidad al permitir que múltiples componentes gaussianas se superpongan y se combinen para representar mejor la verdadera distribución de los datos. Esto los convierte en una herramienta efectiva para modelar datos que pueden ser una mezcla de diferentes poblaciones o grupos subyacentes.</p>
<p>Cada componente gaussiana representa una subdistribución de datos, y se caracteriza por su media (promedio) y desviación estándar (dispersión). En un modelo de mezcla de gaussianas, se asume que cada componente es una distribución normal.</p>
<p>Cada observación en el conjunto se asigna a una de las componentes gaussianas en función de la probabilidad de que pertenezca a esa componente. Esto permite una descripción detallada de cómo se distribuyen los datos entre los diferentes grupos subyacentes.</p>
<p>Los GMM se utilizan en una variedad de aplicaciones, como la segmentación de imágenes, la detección de anomalías, la compresión de datos, la clasificación de patrones y la generación de datos sintéticos.</p>
<p>La estimación de los parámetros de un GMM, es decir, las medias y desviaciones estándar de las componentes gaussianas, se realiza mediante algoritmos de optimización que buscan maximizar la probabilidad conjunta de los datos observados.</p>
<p>Los GMM son especialmente útiles en la detección de anomalías, ya que permiten identificar regiones en el espacio de características donde los datos son inusuales o atípicos en comparación con el modelo GMM.</p>
<p>Aunque los GMM son efectivos en la modelación de datos complejos, pueden ser sensibles al número de componentes elegido y a la inicialización. La selección de un número adecuado de componentes y una buena inicialización son aspectos importantes en su aplicación.</p>
</section>
<section id="comparación-de-modelos" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Comparación de modelos</h1>
<p>Hemos aplicado una variedad de modelos (y en otras asignaturas verás nuevos modelos de ML). Podemos comparar el rendimiento de modelos de diversas maneras. Una de ellas, tal y como vimos en el <a href="eval.html"><span>Capítulo&nbsp;6</span></a> es la curva ROC:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>roc_score</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
roc.default(response = bank.test$deposit, predictor = predicciones,     plot = TRUE, print.auc = TRUE)

Data: predicciones in 1468 controls (bank.test$deposit no) &lt; 1322 cases (bank.test$deposit yes).
Area under the curve: 0.6411</code></pre>
</div>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">pty =</span> <span class="st">"s"</span>) <span class="co"># square</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="fu">roc</span>(df.test<span class="sc">$</span>deposit, probabilities,<span class="at">plot=</span><span class="cn">TRUE</span>, <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">percent =</span> <span class="cn">TRUE</span>, <span class="at">xlab =</span> <span class="st">"Porcentaje Falsos positivos"</span>,</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">"Porcentaje verdaderos postivios"</span>, <span class="at">col =</span> <span class="st">"#377eb8"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">print.auc =</span> <span class="cn">TRUE</span>,<span class="at">legend=</span><span class="cn">TRUE</span>,  <span class="at">brier.in.legend =</span><span class="cn">TRUE</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
roc.default(response = df.test$deposit, predictor = probabilities,     percent = TRUE, plot = TRUE, legacy.axes = TRUE, xlab = "Porcentaje Falsos positivos",     ylab = "Porcentaje verdaderos postivios", col = "#377eb8",     lwd = 2, print.auc = TRUE, legend = TRUE, brier.in.legend = TRUE)

Data: probabilities in 1468 controls (df.test$deposit 0) &lt; 1322 cases (df.test$deposit 1).
Area under the curve: 87.33%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">roc</span>(df.test<span class="sc">$</span>deposit, predicciones, <span class="at">percent=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"#4daf4a"</span>,<span class="at">lwd=</span> <span class="dv">2</span>,</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">print.auc =</span><span class="cn">TRUE</span>, <span class="at">add=</span><span class="cn">TRUE</span>,<span class="at">print.auc.y =</span> <span class="dv">40</span>,<span class="at">plot=</span><span class="cn">TRUE</span>,<span class="at">legend=</span><span class="cn">TRUE</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
roc.default(response = df.test$deposit, predictor = predicciones,     percent = TRUE, plot = TRUE, col = "#4daf4a", lwd = 2, print.auc = TRUE,     add = TRUE, print.auc.y = 40, legend = TRUE)

Data: predicciones in 1468 controls (df.test$deposit 0) &lt; 1322 cases (df.test$deposit 1).
Area under the curve: 64.11%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">roc</span>(df.test<span class="sc">$</span>deposit, prediction.dt, <span class="at">percent=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"goldenrod"</span>,<span class="at">lwd=</span> <span class="dv">2</span>,</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">print.auc =</span><span class="cn">TRUE</span>, <span class="at">add=</span><span class="cn">TRUE</span>,<span class="at">print.auc.y =</span> <span class="dv">30</span>,<span class="at">plot=</span><span class="cn">TRUE</span>,<span class="at">legend=</span><span class="cn">TRUE</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
roc.default(response = df.test$deposit, predictor = prediction.dt,     percent = TRUE, plot = TRUE, col = "goldenrod", lwd = 2,     print.auc = TRUE, add = TRUE, print.auc.y = 30, legend = TRUE)

Data: prediction.dt in 1468 controls (df.test$deposit 0) &lt; 1322 cases (df.test$deposit 1).
Area under the curve: 81.39%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">roc</span>(df.test<span class="sc">$</span>deposit, prediction.rf, <span class="at">percent=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"salmon"</span>,<span class="at">lwd=</span> <span class="dv">2</span>,</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">print.auc =</span><span class="cn">TRUE</span>, <span class="at">add=</span><span class="cn">TRUE</span>,<span class="at">print.auc.y =</span> <span class="dv">20</span>,<span class="at">plot=</span><span class="cn">TRUE</span>,<span class="at">legend=</span><span class="cn">TRUE</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
roc.default(response = df.test$deposit, predictor = prediction.rf,     percent = TRUE, plot = TRUE, col = "salmon", lwd = 2, print.auc = TRUE,     add = TRUE, print.auc.y = 20, legend = TRUE)

Data: prediction.rf in 1468 controls (df.test$deposit 0) &lt; 1322 cases (df.test$deposit 1).
Area under the curve: 87.29%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">roc</span>(df.test<span class="sc">$</span>deposit, prediction.knn, <span class="at">percent=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"#977eb8"</span>,<span class="at">lwd=</span> <span class="dv">2</span>,</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">print.auc =</span><span class="cn">TRUE</span>, <span class="at">add=</span><span class="cn">TRUE</span>,<span class="at">print.auc.y =</span> <span class="dv">10</span>,<span class="at">plot=</span><span class="cn">TRUE</span>,<span class="at">legend=</span><span class="cn">TRUE</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
roc.default(response = df.test$deposit, predictor = prediction.knn,     percent = TRUE, plot = TRUE, col = "#977eb8", lwd = 2, print.auc = TRUE,     add = TRUE, print.auc.y = 10, legend = TRUE)

Data: prediction.knn in 1468 controls (df.test$deposit 0) &lt; 1322 cases (df.test$deposit 1).
Area under the curve: 64.33%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co">#prediction &lt;- predict(tune.fit, df.test, type = 'prob')</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottom"</span>,</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"XGBOOST"</span>, <span class="st">"LOG.REG."</span>, <span class="st">"DT"</span>, <span class="st">"RF"</span>,<span class="st">"KNN"</span>),</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">"#377eb8"</span>, <span class="st">"#4daf4a"</span>, <span class="st">"goldenrod"</span>,<span class="st">"salmon"</span>,<span class="st">"#977eb8"</span>),</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">cex =</span>.<span class="dv">5</span>, <span class="at">xpd =</span> <span class="cn">TRUE</span>, <span class="at">horiz =</span> <span class="cn">TRUE</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="aprsup_files/figure-html/unnamed-chunk-36-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tarea
</div>
</div>
<div class="callout-body-container callout-body">
<p>A la vista de los resultados. ¿Qué modelo es el más adecuado? Piensa sobre ello.</p>
</div>
</div>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Modelo óptimo vs modelo útil
</div>
</div>
<div class="callout-body-container callout-body">
<p>En este documento hemos aplicado una serie de modelos a un conjunto de datos. Nuestro objetivo es mostrar a los estudiantes varios métodos de ML. Nuestro objetivo nunca ha sido encontrar el modelo <em>óptimo</em>. Es decir, no buscamos el mejor modelo, sino un modelo <em>útil</em>. Un modelo que proporcione información y de ella podamos extraer conocimiento.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-agresti2015foundations" class="csl-entry" role="doc-biblioentry">
Agresti, Alan. 2015. <em>Foundations of linear and generalized linear models</em>. John Wiley &amp; Sons.
</div>
<div id="ref-hastie2009elements" class="csl-entry" role="doc-biblioentry">
Hastie, Trevor, Robert Tibshirani, Jerome H Friedman, y Jerome H Friedman. 2009. <em>The elements of statistical learning: data mining, inference, and prediction</em>. Vol. 2. Springer.
</div>
<div id="ref-kelleher2018data" class="csl-entry" role="doc-biblioentry">
Kelleher, John D, y Brendan Tierney. 2018. <em>Data science</em>. MIT Press.
</div>
<div id="ref-mccullagh2019generalized" class="csl-entry" role="doc-biblioentry">
McCullagh, Peter. 2019. <em>Generalized linear models</em>. Routledge.
</div>
<div id="ref-murphy2012machine" class="csl-entry" role="doc-biblioentry">
Murphy, Kevin P. 2012. <em>Machine learning: a probabilistic perspective</em>. MIT press.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiada");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./eval.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Medidas de rendimiento</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./reglas.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Reglas de asociación</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>